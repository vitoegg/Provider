name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  RULES_CONFIG: |
    rules:
      China:
        path: RuleSet/Direct/China.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
      
      Apple:
        path: RuleSet/Apple/Service.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
      
      Reject:
        path: RuleSet/Extra/Reject.list
        urls:
          - https://ruleset.skk.moe/List/domainset/reject.conf
          - https://ruleset.skk.moe/List/domainset/reject_extra.conf
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # 添加日志输出
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        # 添加日志输出
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        # 添加日志输出
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "::group::创建规则处理脚本"
          echo "创建高效的规则处理脚本..."
          # 创建高效处理脚本
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # ==========================================
          # 规则更新脚本 - 自动分析并优化网络规则集合
          # ==========================================

          # 解析规则配置
          # 参数:
          # $1 - 规则配置文本
          # $2 - 规则配置关联数组的引用 (输出)
          # $3 - 规则文件列表数组的引用 (输出)
          parse_rules_config() {
            local config="$1"
            local -n rule_configs_ref="$2"
            local -n rule_files_ref="$3"

            local rule_name=""
            local path=""
            local urls=""

            while IFS= read -r line; do
              # 跳过空行和注释
              [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

              if [[ "$line" =~ ^[[:space:]]*([^:]+):[[:space:]]*$ ]]; then
                rule_name="${BASH_REMATCH[1]}"
                if [[ "$rule_name" != "rules" ]]; then
                  path=""
                  urls=""
                fi
              elif [[ "$line" =~ ^[[:space:]]*path:[[:space:]]*([^[:space:]]+) ]]; then
                path="${BASH_REMATCH[1]}"
                rule_files_ref+=("$path")
              elif [[ "$line" =~ ^[[:space:]]*urls:[[:space:]]*$ ]]; then
                continue
              elif [[ "$line" =~ ^[[:space:]]*-[[:space:]]*([^[:space:]]+) ]]; then
                urls+="${BASH_REMATCH[1]} "
              elif [[ -n "$rule_name" && -n "$path" && -n "$urls" ]]; then
                rule_configs_ref["$rule_name"]="$path;$urls"
                rule_name=""
              fi
            done <<< "$config"

            # 检查最后一个规则
            if [[ -n "$rule_name" && -n "$path" && -n "$urls" && "$rule_name" != "rules" ]]; then
              rule_configs_ref["$rule_name"]="$path;$urls"
            fi
          }

          # 下载并处理规则
          # 参数:
          # $1 - 规则名称
          # $2 - 输出文件路径
          # $3 - 规则源URL列表（空格分隔）
          process_rule() {
            local rule_name="$1"
            local output_path="$2"
            local urls="$3"
            local output_dir
            output_dir=$(dirname "$output_path")
            
            echo "Processing $rule_name to $output_path..."
            TIMEFORMAT='处理用时: %R 秒'
            time {
              # 创建输出目录
              mkdir -p "$output_dir"
              
              # 临时文件
              local merged_file
              local cleaned_file
              merged_file=$(mktemp)
              cleaned_file=$(mktemp)
              
              # 临时目录和并行下载计数
              local tmp_dir=$(mktemp -d)
              local download_count=0
              local download_pids=()
              
              echo "::group::下载规则源文件"
              echo "→ 并行下载规则文件..."
              # 并行下载规则
              for url in $urls; do
                local tmp_file="${tmp_dir}/download_${download_count}"
                (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && echo "  √ 下载成功: $url" || echo "  × 下载失败: $url") &
                download_pids+=($!)
                download_count=$((download_count + 1))
              done
              
              # 等待所有下载完成
              for pid in "${download_pids[@]}"; do
                wait $pid
              done
              echo "::endgroup::"
              
              echo "::group::合并和清理规则"
              echo "→ 合并和清理规则文件..."
              # 合并下载的文件
              find "$tmp_dir" -type f -name "download_*" -exec cat {} \; > "$merged_file"
              
              # 快速清理文件
              LC_ALL=C grep -v "^#\|^$\|^payload:" "$merged_file" > "$cleaned_file" || true
              rm -rf "$tmp_dir"  # 清理临时目录
              
              # 获取数据行数
              local input_count=$(wc -l < "$cleaned_file")
              echo "→ 输入规则数量: $input_count"
              echo "::endgroup::"
              
              # 仅当有内容时才处理域名
              if [[ -s "$cleaned_file" ]]; then
                # 处理格式
                local formatted_file
                formatted_file=$(mktemp)
                
                echo "::group::排序和去重"
                echo "→ 对规则进行排序和去重..."
                # 使用两阶段排序和标记来处理带点前缀域名
                LC_ALL=C awk '
                BEGIN {
                  # 不处理空行和注释
                  FS = "\n";
                  OFS = "\n";
                }
                {
                  # 清理空白
                  gsub(/^[[:space:]]+|[[:space:]]+$/, "", $0);
                  
                  # 跳过空行
                  if (length($0) == 0) next;
                  
                  # 检查是否带前导点
                  if (substr($0, 1, 1) == ".") {
                    # 带前导点的域名，添加标记前缀DOT:，并移除前导点以便排序
                    print "DOT:" substr($0, 2);
                  } else {
                    # 不带前导点的域名，添加标记前缀STD:
                    print "STD:" $0;
                  }
                }
                ' "$cleaned_file" | \
                LC_ALL=C sort | \
                LC_ALL=C awk '
                BEGIN {
                  FS = "\n";
                  last_norm = "";
                  last_prefix = "";
                  last_line = "";
                }
                {
                  # 解析当前行的前缀和规范化域名
                  prefix = substr($0, 1, 4);
                  norm = substr($0, 5);
                  
                  # 如果当前规范化域名与上一行相同
                  if (norm == last_norm && last_norm != "") {
                    # 如果上一行是带点前缀，当前行是不带点
                    if (last_prefix == "DOT:" && prefix == "STD:") {
                      # 跳过当前行（不带点版本）
                      next;
                    }
                    # 如果上一行是不带点，当前行是带点前缀
                    else if (last_prefix == "STD:" && prefix == "DOT:") {
                      # 删除上一行的输出（因为我们还没有实际输出它）
                      # 稍后会输出当前行（带点版本）
                      last_line = "";
                    }
                  }
                  
                  # 输出上一行（如果有）
                  if (last_line != "") {
                    if (last_prefix == "DOT:") {
                      print "." last_norm;
                    } else if (last_prefix == "STD:") {
                      print last_norm;
                    }
                  }
                  
                  # 记住当前行
                  last_norm = norm;
                  last_prefix = prefix;
                  last_line = $0;
                }
                END {
                  # 输出最后一行
                  if (last_line != "") {
                    if (last_prefix == "DOT:") {
                      print "." last_norm;
                    } else if (last_prefix == "STD:") {
                      print last_norm;
                    }
                  }
                }
                ' > "$formatted_file"
                
                local sorted_count=$(wc -l < "$formatted_file")
                echo "→ 排序后规则数量: $sorted_count"
                echo "::endgroup::"
                
                echo "::group::处理域名关系"
                echo "→ 处理域名关系..."
                # 创建最终输出文件
                local final_file
                final_file=$(mktemp)
                
                # 根据文件类型添加头部
                {
                  if [[ "$output_path" == *.yaml ]]; then
                    echo "payload:"
                  fi
                  
                  # 处理父子域名关系
                  LC_ALL=C awk '
                  function domain_is_subdomain(sub, parent,    sub_parts, parent_parts, i, sub_len, parent_len) {
                    # 如果子域名比父域名短，肯定不是子域名
                    if (length(sub) <= length(parent)) return 0;
                    
                    # 子域名必须以父域名结尾
                    if (substr(sub, length(sub) - length(parent) + 1) != parent) return 0;
                    
                    # 父域名前面必须是点
                    if (substr(sub, length(sub) - length(parent), 1) != ".") return 0;
                    
                    return 1;
                  }
                  
                  # 预处理：读取所有域名到数组
                  {
                    # 去除前导点进行比较
                    original = $0;
                    normalized = original;
                    if (substr(normalized, 1, 1) == ".") {
                      normalized = substr(normalized, 2);
                    }
                    
                    domains[++count] = original;
                    norm_domains[count] = normalized;
                  }
                  
                  END {
                    # 按标准化域名长度进行排序（从短到长）
                    for (i = 1; i <= count; i++) {
                      len = length(norm_domains[i]);
                      idx_by_len[len, ++len_count[len]] = i;
                    }
                    
                    # 按长度对标准化域名进行排序
                    for (len in len_count) {
                      lens[++lcount] = len;
                    }
                    
                    # 冒泡排序长度数组
                    for (i = 1; i < lcount; i++) {
                      for (j = i + 1; j <= lcount; j++) {
                        if (lens[i] > lens[j]) {
                          temp = lens[i];
                          lens[i] = lens[j];
                          lens[j] = temp;
                        }
                      }
                    }
                    
                    # 处理父子域名关系
                    # 由短到长处理，确保父域名先被处理
                    for (li = 1; li <= lcount; li++) {
                      curr_len = lens[li];
                      
                      # 处理当前长度的所有域名
                      for (i = 1; i <= len_count[curr_len]; i++) {
                        idx = idx_by_len[curr_len, i];
                        
                        # 如果已被标记为删除，跳过
                        if (idx in to_remove) continue;
                        
                        curr_norm = norm_domains[idx];
                        
                        # 与所有更长的域名比较
                        for (lj = li + 1; lj <= lcount; lj++) {
                          longer_len = lens[lj];
                          
                          for (j = 1; j <= len_count[longer_len]; j++) {
                            longer_idx = idx_by_len[longer_len, j];
                            
                            # 如果已被标记为删除，跳过
                            if (longer_idx in to_remove) continue;
                            
                            longer_norm = norm_domains[longer_idx];
                            
                            # 检查是否是子域名关系
                            if (domain_is_subdomain(longer_norm, curr_norm)) {
                              # 标记子域名为删除
                              to_remove[longer_idx] = 1;
                            }
                          }
                        }
                      }
                    }
                    
                    # 输出最终结果
                    for (i = 1; i <= count; i++) {
                      if (!(i in to_remove)) {
                        print domains[i];
                      }
                    }
                  }
                  ' "$formatted_file"
                } > "$final_file"
                
                # 获取处理后结果数量
                local output_count=$(wc -l < "$final_file")
                echo "→ 处理后规则数量: $output_count (减少了 $((sorted_count - output_count)) 条冗余规则)" 
                echo "::endgroup::"
                
                echo "::group::生成最终规则文件"
                # 创建带有元数据的版本
                local meta_file
                meta_file=$(mktemp)
                
                {
                  echo "# Merged from:"
                  for url in $urls; do
                    repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                    echo "# - https://$repo_url"
                  done
                  echo ""
                  cat "$final_file"
                } > "$meta_file"
                
                # 检查是否有更改
                local changed=0
                if [ -f "$output_path" ]; then
                  local old_file
                  old_file=$(mktemp)
                  grep -v "^# Update time:" "$output_path" > "$old_file"
                  
                  if ! cmp -s "$old_file" "$meta_file"; then
                    changed=1
                  fi
                  rm -f "$old_file"
                else
                  changed=1
                fi
                
                # 如果有更改，写入新文件
                if [ $changed -eq 1 ]; then
                  {
                    echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                    cat "$meta_file"
                  } > "$output_path"
                  echo "→ 检测到规则变更，已更新文件"
                else
                  echo "→ 未检测到规则变更"
                fi
                echo "::endgroup::"
                
                # 清理临时文件
                rm -f "$formatted_file" "$final_file" "$meta_file"
              else
                echo "→ 警告: $rule_name 没有有效内容"
              fi
              
              # 清理主要临时文件
              rm -f "$merged_file" "$cleaned_file"
            }
          }

          # 主函数 - 协调整个规则更新过程
          # 参数:
          # $1 - 完整的规则配置
          main() {
            local config="$1"
            
            # 初始化数据结构
            declare -A rule_configs
            declare -a rule_files
            
            # 解析配置
            parse_rules_config "$config" rule_configs rule_files
            
            echo "==============================="
            echo "开始更新规则文件 ($(date '+%Y-%m-%d %H:%M:%S'))"
            echo "==============================="
            
            # 记录总体开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule in "${!rule_configs[@]}"; do
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule]}"
              echo "==============================="
              echo "::group::处理规则: $rule"
              process_rule "$rule" "$output_path" "$urls"
              echo "::endgroup::"
              echo "==============================="
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "::group::处理总结"
            echo "所有规则处理完成，总耗时: $((duration / 60))分$((duration % 60))秒"
            
            # 性能检查
            if [ $duration -gt 180 ]; then
              echo "警告: 处理时间超过3分钟，建议进一步优化性能"
            elif [ $duration -lt 30 ]; then
              echo "性能良好: 处理时间少于30秒"
            else
              echo "性能正常: 处理时间在目标范围内"
            fi
            echo "::endgroup::"
            
            # 检查Git变更
            local changes_detected=false
            
            git add "${rule_files[@]}" 2>/dev/null || true
            
            echo "::group::检查文件变更"
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                changes_detected=true
                echo "检测到变更: $file"
              fi
            done
            echo "::endgroup::"
            
            # 输出结果
            if [ "$changes_detected" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "检测到规则文件变更"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "未检测到规则文件变更"
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
          }

          # 运行主函数
          main "$1"
          EOF
          
          chmod +x process_rules.sh
          echo "脚本创建完成"
          echo "::endgroup::"

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "::group::开始更新规则集"
          echo "开始规则处理流程..."
          ./process_rules.sh "${{ env.RULES_CONFIG }}"
          echo "::endgroup::"

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "::group::提交变更"
          echo "检测到规则变更，提交到仓库..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Auto update rules by action bot"
          
          # 显示本次提交的统计信息但不显示具体内容
          echo "本次更新统计:"
          git show --stat --oneline HEAD
          
          # 推送更改
          echo "推送更改到远程仓库..."
          git push
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 6  # 保留最近6次运行记录，便于查看历史
