name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  RULES_CONFIG: |
    rules:
      China:
        path: RuleSet/Direct/China.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
      
      Apple:
        path: RuleSet/Apple/Service.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
      
      Reject:
        path: RuleSet/Extra/Reject.list
        urls:
          - https://ruleset.skk.moe/List/domainset/reject.conf
          - https://ruleset.skk.moe/List/domainset/reject_extra.conf
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # 添加日志输出
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        # 添加日志输出
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        # 添加日志输出
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "::group::创建规则处理脚本"
          echo "创建高效的规则处理脚本..."
          # 创建高效处理脚本
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # ==========================================
          # 规则更新脚本 - 自动分析并优化网络规则集合
          # ==========================================

          # 解析规则配置
          # 参数:
          # $1 - 规则配置文本
          # $2 - 规则配置关联数组的引用 (输出)
          # $3 - 规则文件列表数组的引用 (输出)
          parse_rules_config() {
            local config="$1"
            local -n rule_configs_ref="$2"
            local -n rule_files_ref="$3"

            local rule_name=""
            local path=""
            local urls=""

            while IFS= read -r line; do
              # 跳过空行和注释
              [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

              if [[ "$line" =~ ^[[:space:]]*([^:]+):[[:space:]]*$ ]]; then
                rule_name="${BASH_REMATCH[1]}"
                if [[ "$rule_name" != "rules" ]]; then
                  path=""
                  urls=""
                fi
              elif [[ "$line" =~ ^[[:space:]]*path:[[:space:]]*([^[:space:]]+) ]]; then
                path="${BASH_REMATCH[1]}"
                rule_files_ref+=("$path")
              elif [[ "$line" =~ ^[[:space:]]*urls:[[:space:]]*$ ]]; then
                continue
              elif [[ "$line" =~ ^[[:space:]]*-[[:space:]]*([^[:space:]]+) ]]; then
                urls+="${BASH_REMATCH[1]} "
              elif [[ -n "$rule_name" && -n "$path" && -n "$urls" ]]; then
                rule_configs_ref["$rule_name"]="$path;$urls"
                rule_name=""
              fi
            done <<< "$config"

            # 检查最后一个规则
            if [[ -n "$rule_name" && -n "$path" && -n "$urls" && "$rule_name" != "rules" ]]; then
              rule_configs_ref["$rule_name"]="$path;$urls"
            fi
          }

          # 下载并处理规则
          # 参数:
          # $1 - 规则名称
          # $2 - 输出文件路径
          # $3 - 规则源URL列表（空格分隔）
          process_rule() {
            local rule_name="$1"
            local output_path="$2"
            local urls="$3"
            local output_dir
            output_dir=$(dirname "$output_path")
            
            echo "Processing $rule_name to $output_path..."
            TIMEFORMAT='处理用时: %R 秒'
            time {
              # 创建输出目录
              mkdir -p "$output_dir"
              
              # 临时文件
              local merged_file
              local cleaned_file
              merged_file=$(mktemp)
              cleaned_file=$(mktemp)
              
              # 临时目录和并行下载计数
              local tmp_dir=$(mktemp -d)
              local download_count=0
              local download_pids=()
              
              echo "::group::下载规则源文件"
              echo "→ 并行下载规则文件..."
              # 并行下载规则
              for url in $urls; do
                local tmp_file="${tmp_dir}/download_${download_count}"
                (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && echo "  √ 下载成功: $url" || echo "  × 下载失败: $url") &
                download_pids+=($!)
                download_count=$((download_count + 1))
              done
              
              # 等待所有下载完成
              for pid in "${download_pids[@]}"; do
                wait $pid
              done
              echo "::endgroup::"
              
              echo "::group::合并和清理规则"
              echo "→ 合并和清理规则文件..."
              # 合并下载的文件
              find "$tmp_dir" -type f -name "download_*" -exec cat {} \; > "$merged_file"
              
              # 1. 清理规则：移除注释、空行、行首尾空格、payload:
              LC_ALL=C sed -e 's/[[:space:]]*#.*$//' \\
                           -e 's/^[[:space:]]*//;s/[[:space:]]*$//' \\
                           -e '/^$/d' \\
                           -e '/^[[:space:]]*#/d' \\
                           -e '/^payload:/d' "$merged_file" > "$cleaned_file" || true
              rm -rf "$tmp_dir"  # 清理临时目录
              
              local input_count=$(wc -l < "$cleaned_file")
              echo "→ 清理后输入规则数量: $input_count"
              echo "::endgroup::"
              
              # 仅当有内容时才处理域名
              if [[ -s "$cleaned_file" ]]; then
                local sorted_unique_file
                sorted_unique_file=$(mktemp)
                
                echo "::group::排序和初步去重"
                echo "→ 对规则进行排序和基础去重..."
                LC_ALL=C sort -u "$cleaned_file" > "$sorted_unique_file"
                local sorted_count=$(wc -l < "$sorted_unique_file")
                echo "→ 初步去重后规则数量: $sorted_count"
                echo "::endgroup::"

                echo "::group::处理父子域名去重"
                echo "→ 根据父子域名关系进一步去重..."
                local final_file
                final_file=$(mktemp)
                
                LC_ALL=C awk \'
                # AWK script for final deduplication (parent/child domains)
                # Input: cleaned and unique-sorted lines from sorted_unique_file

                function normalize_domain(domain) {
                    if (substr(domain, 1, 1) == ".") {
                        return substr(domain, 2);
                    }
                    return domain;
                }

                function is_parent_of(p_candidate, c_candidate,    p_is_dot, c_is_dot, p_norm, c_norm) {
                    p_is_dot = (substr(p_candidate, 1, 1) == ".");
                    c_is_dot = (substr(c_candidate, 1, 1) == ".");
                    
                    p_norm = normalize_domain(p_candidate);
                    c_norm = normalize_domain(c_candidate);

                    # Rule a: .abc.com is parent of abc.com
                    if (p_norm == c_norm) {
                        if (p_is_dot && !c_is_dot) return 1; # .abc.com is parent of abc.com
                        return 0; 
                    }

                    # Rule b: abc.com is parent of 1.abc.com (and by extension .abc.com is parent of 1.abc.com / .1.abc.com)
                    if (length(c_norm) > length(p_norm) && \\
                        substr(c_norm, length(c_norm) - length(p_norm)) == p_norm && \\
                        substr(c_norm, length(c_norm) - length(p_norm) - 1, 1) == ".") {
                        return 1; # p_norm is structural parent of c_norm, so p_candidate is parent of c_candidate
                    }
                    return 0;
                }

                {
                    lines[++n] = $0;
                }

                END {
                    for (i = 1; i <= n; i++) {
                        is_child = 0;
                        current_line = lines[i];
                        for (j = 1; j <= n; j++) {
                            if (i == j) continue;
                            potential_parent = lines[j];
                            if (is_parent_of(potential_parent, current_line)) {
                                is_child = 1;
                                break;
                            }
                        }
                        if (!is_child) {
                            print current_line;
                        }
                    }
                }
                \' "$sorted_unique_file" > "$final_file"
                
                rm -f "$sorted_unique_file" # Clean up sorted_unique_file

                local output_count=$(wc -l < "$final_file")
                echo "→ 处理后规则数量: $output_count (减少了 $((sorted_count - output_count)) 条冗余规则)" 
                echo "::endgroup::"
                
                echo "::group::生成最终规则文件"
                # 创建带有元数据的版本
                local meta_file
                meta_file=$(mktemp)
                
                {
                  echo "# Merged from:"
                  for url in $urls; do
                    repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                    echo "# - https://$repo_url"
                  done
                  echo ""
                  cat "$final_file"
                } > "$meta_file"
                
                # 检查是否有更改
                local changed=0
                if [ -f "$output_path" ]; then
                  local old_file
                  old_file=$(mktemp)
                  grep -v "^# Update time:" "$output_path" > "$old_file"
                  
                  if ! cmp -s "$old_file" "$meta_file"; then
                    changed=1
                  fi
                  rm -f "$old_file"
                else
                  changed=1
                fi
                
                # 如果有更改，写入新文件
                if [ $changed -eq 1 ]; then
                  {
                    echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                    cat "$meta_file"
                  } > "$output_path"
                  echo "→ 检测到规则变更，已更新文件"
                else
                  echo "→ 未检测到规则变更"
                fi
                echo "::endgroup::"
                
                # 清理临时文件
                rm -f "$final_file" "$meta_file"
              else
                echo "→ 警告: $rule_name 没有有效内容"
              fi
              
              # 清理主要临时文件
              rm -f "$merged_file" "$cleaned_file"
            }
          }

          # 主函数 - 协调整个规则更新过程
          # 参数:
          # $1 - 完整的规则配置
          main() {
            local config="$1"
            
            # 初始化数据结构
            declare -A rule_configs
            declare -a rule_files
            
            # 解析配置
            parse_rules_config "$config" rule_configs rule_files
            
            echo "==============================="
            echo "开始更新规则文件 ($(date '+%Y-%m-%d %H:%M:%S'))"
            echo "==============================="
            
            # 记录总体开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule in "${!rule_configs[@]}"; do
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule]}"
              echo "==============================="
              echo "::group::处理规则: $rule"
              process_rule "$rule" "$output_path" "$urls"
              echo "::endgroup::"
              echo "==============================="
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "::group::处理总结"
            echo "所有规则处理完成，总耗时: $((duration / 60))分$((duration % 60))秒"
            
            # 性能检查
            if [ $duration -gt 180 ]; then
              echo "警告: 处理时间超过3分钟，建议进一步优化性能"
            elif [ $duration -lt 30 ]; then
              echo "性能良好: 处理时间少于30秒"
            else
              echo "性能正常: 处理时间在目标范围内"
            fi
            echo "::endgroup::"
            
            # 检查Git变更
            local changes_detected=false
            
            git add "${rule_files[@]}" 2>/dev/null || true
            
            echo "::group::检查文件变更"
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                changes_detected=true
                echo "检测到变更: $file"
              fi
            done
            echo "::endgroup::"
            
            # 输出结果
            if [ "$changes_detected" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "检测到规则文件变更"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "未检测到规则文件变更"
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
          }

          # 运行主函数
          main "$1"
          EOF
          
          chmod +x process_rules.sh
          echo "脚本创建完成"
          echo "::endgroup::"

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "::group::开始更新规则集"
          echo "开始规则处理流程..."
          ./process_rules.sh "${{ env.RULES_CONFIG }}"
          echo "::endgroup::"

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "::group::提交变更"
          echo "检测到规则变更，提交到仓库..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Auto update rules by action bot"
          
          # 显示本次提交的统计信息但不显示具体内容
          echo "本次更新统计:"
          git show --stat --oneline HEAD
          
          # 推送更改
          echo "推送更改到远程仓库..."
          git push
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 6  # 保留最近6次运行记录，便于查看历史
