name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  RULES_CONFIG: |
    rules:
      China:
        path: RuleSet/Direct/China.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
      
      Apple:
        path: RuleSet/Apple/Service.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
      
      Reject:
        path: RuleSet/Extra/Reject.list
        urls:
          - https://ruleset.skk.moe/List/domainset/reject.conf
          - https://ruleset.skk.moe/List/domainset/reject_extra.conf
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # 添加日志输出
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        # 添加日志输出
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        # 添加日志输出
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "::group::创建规则处理脚本"
          echo "创建高效的规则处理脚本..."
          # 创建高效处理脚本
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # ==========================================
          # 规则更新脚本 - 自动分析并优化网络规则集合
          # ==========================================

          # 解析规则配置
          # 参数:
          # $1 - 规则配置文本
          # $2 - 规则配置关联数组的引用 (输出)
          # $3 - 规则文件列表数组的引用 (输出)
          parse_rules_config() {
            local config="$1"
            local -n rule_configs_ref="$2"
            local -n rule_files_ref="$3"

            local rule_name=""
            local path=""
            local urls=""

            while IFS= read -r line; do
              # 跳过空行和注释
              [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

              if [[ "$line" =~ ^[[:space:]]*([^:]+):[[:space:]]*$ ]]; then
                rule_name="${BASH_REMATCH[1]}"
                if [[ "$rule_name" != "rules" ]]; then
                  path=""
                  urls=""
                fi
              elif [[ "$line" =~ ^[[:space:]]*path:[[:space:]]*([^[:space:]]+) ]]; then
                path="${BASH_REMATCH[1]}"
                rule_files_ref+=("$path")
              elif [[ "$line" =~ ^[[:space:]]*urls:[[:space:]]*$ ]]; then
                continue
              elif [[ "$line" =~ ^[[:space:]]*-[[:space:]]*([^[:space:]]+) ]]; then
                urls+="${BASH_REMATCH[1]} "
              elif [[ -n "$rule_name" && -n "$path" && -n "$urls" ]]; then
                rule_configs_ref["$rule_name"]="$path;$urls"
                rule_name=""
              fi
            done <<< "$config"

            # 检查最后一个规则
            if [[ -n "$rule_name" && -n "$path" && -n "$urls" && "$rule_name" != "rules" ]]; then
              rule_configs_ref["$rule_name"]="$path;$urls"
            fi
          }

          # 下载并处理规则
          # 参数:
          # $1 - 规则名称
          # $2 - 输出文件路径
          # $3 - 规则源URL列表（空格分隔）
          process_rule() {
            local rule_name="$1"      # 当前处理的规则集名称
            local output_path="$2"    # 规则输出路径
            local urls="$3"           # 空格分隔的URL列表
            
            # 获取输出目录
            local output_dir
            output_dir=$(dirname "$output_path")
            
            echo "Processing $rule_name to $output_path..."
            # 输出处理时间统计
            TIMEFORMAT='处理用时: %R 秒'
            time (
              # 创建输出目录（如果不存在）
              mkdir -p "$output_dir"
              
              # === 设置处理所需的临时文件 ===
              local merged_file=$(mktemp "/tmp/rule_merged_${rule_name//[^a-zA-Z0-9]/_}.XXXXXX")
              local cleaned_file=$(mktemp "/tmp/rule_cleaned_${rule_name//[^a-zA-Z0-9]/_}.XXXXXX")
              
              # 创建临时下载目录
              local tmp_dir=$(mktemp -d "/tmp/rule_download_${rule_name//[^a-zA-Z0-9]/_}.XXXXXX")
              local download_count=0
              local download_pids=()
              
              echo "::group::下载规则源文件"
              echo "→ 并行下载规则文件..."
              # 并行下载规则
              for url in $urls; do
                local tmp_file="${tmp_dir}/download_${download_count}"
                (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && echo "  √ 下载成功: $url" || echo "  × 下载失败: $url") &
                download_pids+=($!)
                download_count=$((download_count + 1))
              done
              
              # 等待所有下载完成
              for pid in "${download_pids[@]}"; do
                wait $pid
              done
              echo "::endgroup::"
              
              echo "::group::合并和清理规则"
              echo "→ 合并和清理规则文件..."
              # 合并下载的文件
              find "$tmp_dir" -type f -name "download_*" -exec cat {} \; > "$merged_file"
              
              # === 规则清理流程 ===
              # 按顺序执行一系列清理操作，确保规则标准化和干净
              echo "→ 执行规则清理操作..."
              LC_ALL=C sed -e 's/[[:space:]]*#.*$//'        `# 1. 移除每行末尾的注释` \
                         -e 's/^[[:space:]]*//;s/[[:space:]]*$//' `# 2. 移除行首尾空白字符` \
                         -e '/^$/d'                         `# 3. 删除空行` \
                         -e '/^[[:space:]]*#/d'             `# 4. 删除纯注释行` \
                         -e '/^payload:/d'                  `# 5. 删除payload标记行` \
                         "$merged_file" > "$cleaned_file" || true
              
              # 清理临时下载目录节省空间
              rm -rf "$tmp_dir"
              
              # 统计清理后的规则数量
              local input_count=$(wc -l < "$cleaned_file")
              echo "→ 清理后输入规则数量: $input_count"
              echo "::endgroup::"
              
              # 仅当有内容时才处理域名
              if [[ -s "$cleaned_file" ]]; then
                local sorted_unique_file
                sorted_unique_file=$(mktemp)
                
                echo "::group::排序和初步去重"
                echo "→ 对规则进行排序和基础去重..."
                # 使用LC_ALL=C确保排序一致性，提高性能
                LC_ALL=C sort -u "$cleaned_file" > "$sorted_unique_file"
                local sorted_count=$(wc -l < "$sorted_unique_file")
                echo "→ 初步去重后规则数量: $sorted_count"
                echo "::endgroup::"

                echo "::group::处理父子域名去重"
                echo "→ 根据父子域名关系进一步去重..."
                local final_file
                final_file=$(mktemp)
                
                # 使用AWK进行高效的父子域名去重分析
                LC_ALL=C awk '
                # === 域名规则去重处理程序 ===
                # 用途：分析并删除被父域名包含的子域名规则
                # 策略：两阶段处理 - 1.收集和预处理数据 2.执行高效去重算法

                BEGIN {
                  # 初始化数据结构，预分配内存以优化性能
                  split("", domains);      # 存储规范化后的域名
                  split("", is_dot_prefix); # 记录域名是否有点前缀
                  # 注意：预分配可减少动态分配的内存开销，提高大数据集处理性能
                }

                # === 域名规范化函数 ===
                # 作用：统一处理域名格式，移除前导点
                # 输入：原始域名
                # 输出：规范化的域名字符串
                function normalize_domain(domain) {
                  # 检测并移除前导点
                  if (substr(domain, 1, 1) == ".") {
                    return substr(domain, 2); # 返回无点前缀的域名
                  }
                  return domain; # 原样返回无前导点的域名
                }

                # === 父域名检测函数 ===
                # 作用：判断一个域名是否是另一个域名的父域名
                # 输入：两个域名字符串 - 潜在父域名和潜在子域名
                # 输出：布尔值，1表示是父域名，0表示不是
                function is_parent_of(p_candidate, c_candidate) {
                  # 高效获取预处理的域名属性
                  local p_is_dot = is_dot_prefix[p_candidate]; # 父域名是否有点前缀
                  local c_is_dot = is_dot_prefix[c_candidate]; # 子域名是否有点前缀
                  local p_norm = domains[p_candidate];         # 规范化后的父域名
                  local c_norm = domains[c_candidate];         # 规范化后的子域名

                  # === 规则A：点前缀优先级规则 ===
                  # 情况：相同域名但一个有点前缀
                  # 例如：.example.com 是 example.com 的父域名
                  if (p_norm == c_norm) {
                    if (p_is_dot && !c_is_dot) return 1; # 前导点版本优先
                    return 0; # 其他情况非父子关系
                  }

                  # === 规则B：域名层级规则 ===
                  # 情况：检查一个域名是否是另一个的顶级域名
                  # 例如：example.com 是 sub.example.com 的父域名
                  if (length(c_norm) > length(p_norm) && 
                      # 检查c_norm是否以p_norm结尾
                      substr(c_norm, length(c_norm) - length(p_norm) + 1) == p_norm && 
                      # 确保在p_norm前有一个点（表示子域关系）
                      substr(c_norm, length(c_norm) - length(p_norm), 1) == ".") {
                    return 1; # 确认是父域名关系
                  }
                  
                  return 0; # 默认情况：不是父子域名关系
                }

                # === 第一阶段：数据收集与预处理 ===
                # 作用：读取每一行规则，进行预处理
                {
                  lines[NR] = $0; # 按原样存储每行规则
                  
                  # 预计算并存储每个域名的关键属性
                  is_dot_prefix[$0] = (substr($0, 1, 1) == "."); # 是否有点前缀
                  domains[$0] = normalize_domain($0);            # 存储规范化域名
                }

                # === 第二阶段：智能去重算法 ===
                # 执行高效的父子域名识别和去重
                END {
                  # 使用分离的数组存储子域名标记
                  split("", is_child);
                  
                  # 第一步：标记所有子域名
                  for (i = 1; i <= NR; i++) {
                    # 性能优化：已知是子域名的跳过后续比较
                    if (is_child[i]) continue;
                    
                    current = lines[i]; # 当前检查的域名
                    
                    # 与所有其他域名比较
                    for (j = 1; j <= NR; j++) {
                      if (i == j) continue; # 跳过自身比较
                      
                      potential_parent = lines[j]; # 潜在父域名
                      
                      # 如果发现是子域名，标记并停止比较
                      if (is_parent_of(potential_parent, current)) {
                        is_child[i] = 1; # 标记为子域名
                        break; # 找到一个父域名就足够，无需继续比较
                      }
                    }
                  }
                  
                  # 第二步：只输出非子域名（即父域名或独立域名）
                  for (i = 1; i <= NR; i++) {
                    if (!is_child[i]) {
                      print lines[i]; # 输出需要保留的规则
                    }
                  }
                }
                ' "$sorted_unique_file" > "$final_file"
                
                # 清理临时文件
                rm -f "$sorted_unique_file"

                # 输出处理结果统计
                local output_count=$(wc -l < "$final_file")
                local removed_count=$((sorted_count - output_count))
                echo "→ 处理后规则数量: $output_count (减少了 $removed_count 条冗余规则)" 
                echo "::endgroup::"
                
                echo "::group::生成最终规则文件"
                # 创建带有元数据的版本
                local meta_file
                meta_file=$(mktemp)
                
                {
                  echo "# Merged from:"
                  for url in $urls; do
                    repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                    echo "# - https://$repo_url"
                  done
                  echo ""
                  cat "$final_file"
                } > "$meta_file"
                
                # 检查是否有更改
                local changed=0
                if [ -f "$output_path" ]; then
                  local old_file
                  old_file=$(mktemp)
                  grep -v "^# Update time:" "$output_path" > "$old_file"
                  
                  if ! cmp -s "$old_file" "$meta_file"; then
                    changed=1
                  fi
                  rm -f "$old_file"
                else
                  changed=1
                fi
                
                # 如果有更改，写入新文件
                if [ $changed -eq 1 ]; then
                  {
                    echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                    cat "$meta_file"
                  } > "$output_path"
                  echo "→ 检测到规则变更，已更新文件"
                else
                  echo "→ 未检测到规则变更"
                fi
                echo "::endgroup::"
                
                # 清理临时文件
                rm -f "$final_file" "$meta_file"
              else
                echo "→ 警告: $rule_name 没有有效内容"
              fi
              
              # 清理主要临时文件
              rm -f "$merged_file" "$cleaned_file"
            )
          }

          # 主函数 - 协调整个规则更新过程
          # 参数:
          # $1 - 完整的规则配置
          main() {
            local config="$1"
            
            # === 初始化阶段 ===
            # 声明关联数组存储规则配置，普通数组存储规则文件列表
            declare -A rule_configs  # 关联数组：规则名称 -> 路径和URL
            declare -a rule_files    # 普通数组：所有规则文件路径列表
            
            # 解析输入的YAML配置
            echo "→ 解析规则配置..."
            parse_rules_config "$config" rule_configs rule_files
            
            echo "==============================="
            echo "开始更新规则文件 ($(date '+%Y-%m-%d %H:%M:%S'))"
            echo "==============================="
            
            # 记录总处理开始时间
            local start_time=$SECONDS
            
            # === 规则处理阶段 ===
            # 依次处理每个规则集
            echo "→ 共发现 ${#rule_configs[@]} 个规则集需要处理"
            for rule in "${!rule_configs[@]}"; do
              # 提取规则的路径和URL列表
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule]}"
              echo "==============================="
              echo "::group::处理规则: $rule"
              # 调用处理函数处理单个规则
              process_rule "$rule" "$output_path" "$urls"
              echo "::endgroup::"
              echo "==============================="
            done
            
            # === 总结阶段 ===
            # 计算总耗时并评估性能
            local duration=$((SECONDS - start_time))
            echo "::group::处理总结"
            echo "所有规则处理完成，总耗时: $((duration / 60))分$((duration % 60))秒"
            
            # 性能评估
            if [ $duration -gt 180 ]; then
              echo "⚠️ 警告: 处理时间超过3分钟，建议进一步优化性能"
            elif [ $duration -lt 30 ]; then
              echo "✅ 性能良好: 处理时间少于30秒"
            else
              echo "✓ 性能正常: 处理时间在目标范围内"
            fi
            echo "::endgroup::"
            
            # === 变更检测阶段 ===
            # 检查是否有规则文件发生变化
            local changes_detected=false
            
            # 将所有规则文件添加到Git暂存区，用于检测变更
            git add "${rule_files[@]}" 2>/dev/null || true
            
            echo "::group::检查文件变更"
            for file in "${rule_files[@]}"; do
              # 检查文件是否存在且有除更新时间外的实质性变更
              if [ -f "$file" ] && git diff --cached --no-color "$file" | grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                changes_detected=true
                echo "检测到变更: $file"
              fi
            done
            echo "::endgroup::"
            
            # === 输出结果 ===
            # 设置GitHub Actions输出变量并输出结果
            if [ "$changes_detected" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "✨ 检测到规则文件变更，将在后续步骤提交更改"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "ℹ️ 未检测到规则文件变更，无需提交"
              # 恢复已暂存的文件
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
          }

          # 运行主函数
          main "$1"
          EOF
          
          chmod +x process_rules.sh
          echo "脚本创建完成"
          echo "::endgroup::"

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "::group::开始更新规则集"
          echo "开始规则处理流程..."
          ./process_rules.sh "${{ env.RULES_CONFIG }}"
          echo "::endgroup::"

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "::group::提交变更"
          echo "检测到规则变更，提交到仓库..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Auto update rules by action bot"
          
          # 显示本次提交的统计信息但不显示具体内容
          echo "本次更新统计:"
          git show --stat --oneline HEAD
          
          # 推送更改
          echo "推送更改到远程仓库..."
          git push
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 6  # 保留最近6次运行记录，便于查看历史
