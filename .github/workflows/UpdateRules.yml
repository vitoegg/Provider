name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  # 规则配置 - 简单格式
  # 格式: RULE_名称_PATH=输出路径 和 RULE_名称_URLS=URL1 URL2 URL3
  
  # China规则
  RULE_CHINA_PATH: RuleSet/Direct/China.list
  RULE_CHINA_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
  
  # Apple规则
  RULE_APPLE_PATH: RuleSet/Apple/Service.list
  RULE_APPLE_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
  
  # Reject规则
  RULE_REJECT_PATH: RuleSet/Extra/Reject.list
  RULE_REJECT_URLS: >-
    https://ruleset.skk.moe/List/domainset/reject.conf
    https://ruleset.skk.moe/List/domainset/reject_extra.conf
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "开始生成规则处理脚本..."
          echo "脚本功能: 下载规则、清理规则、合并规则、统计变化"
          echo "脚本结构: 3个主要函数 - 配置读取、规则处理、主程序"
          
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # =====================================
          # 网络规则更新脚本
          # 功能: 下载、清理、去掉重复的规则
          # =====================================

          # 第一步: 从环境变量获取规则配置信息
          get_rules_config() {
            # 接收参数
            local -n rule_list="$1"   # 存放规则配置的数组
            local -n file_list="$2"   # 存放文件路径的数组
            
            echo "正在查找规则配置..."
            
            # 查找所有规则变量 (格式: RULE_规则名_PATH)
            for var in $(env | grep "^RULE_.*_PATH=" | cut -d= -f1); do
              # 从变量名中提取规则名称
              local rule_name=${var%_PATH}   # 去掉_PATH后缀
              rule_name=${rule_name#RULE_}   # 去掉RULE_前缀
              
              # 获取对应的URL变量名 (格式: RULE_规则名_URLS)
              local url_var="RULE_${rule_name}_URLS"
              
              # 检查URL变量是否存在
              if [[ -v $url_var ]]; then
                # 获取路径和URL值
                local path=${!var}             # 获取路径变量的值
                local urls=${!url_var}         # 获取URL变量的值
                
                # 显示找到的规则信息
                echo "找到规则: $rule_name"
                echo "- 保存路径: $path"
                echo "- 下载地址数量: $(echo "$urls" | wc -w)"
                
                # 保存规则信息到数组中
                rule_list["$rule_name"]="$path;$urls"  # 路径和URL用分号分隔
                file_list+=("$path")                   # 添加到文件列表
              fi
            done
            
            # 显示找到的规则总数
            echo "共找到 ${#rule_list[@]} 个规则"
          }

          # 第二步: 处理单个规则
          process_rule() {
            # 接收参数
            local rule_name="$1"      # 规则名称
            local output_path="$2"    # 输出文件路径
            local urls="$3"           # URL列表
            
            # 创建输出目录(如果不存在)
            local output_dir=$(dirname "$output_path")
            mkdir -p "$output_dir"
            
            echo "开始处理规则: $rule_name"
            echo "保存位置: $output_path"
            
            # 计时
            local start_time=$SECONDS
            
            # 1. 下载规则文件
            echo "正在下载规则..."
            
            # 创建临时文件
            local merged_file=$(mktemp)  # 合并后的文件
            local cleaned_file=$(mktemp) # 清理后的文件
            local tmp_dir=$(mktemp -d)   # 临时下载目录
            
            # 并行下载所有URL
            local download_count=0
            local download_pids=()
            
            for url in $urls; do
              # 为每个URL创建临时文件
              local tmp_file="${tmp_dir}/download_${download_count}"
              
              # 后台下载并显示结果
              (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && 
               echo "下载成功: $url" || 
               echo "下载失败: $url") &
              
              # 保存进程ID和计数
              download_pids+=($!)
              download_count=$((download_count + 1))
            done
            
            # 等待所有下载完成
            for pid in "${download_pids[@]}"; do
              wait $pid
            done
            
            # 2. 合并和清理规则
            echo "正在合并和清理规则..."
            
            # 合并所有下载的文件
            cat "${tmp_dir}"/download_* > "$merged_file"
            
            # 清理规则:
            # - 删除注释和空行
            # - 删除行首尾空格
            sed -e 's/[[:space:]]*[#;\/\/].*$//' \
                -e 's/^[[:space:]]*//;s/[[:space:]]*$//' \
                -e '/^$/d' \
                -e '/^[[:space:]]*[#;\/\/]/d' \
                -e '/^payload:/d' \
                -e '/^[[:space:]]*\/\*/d;/\*\//d' \
                "$merged_file" > "$cleaned_file"
            
            # 统计清理后的规则数量
            local cleaned_count=$(wc -l < "$cleaned_file")
            echo "清理后的规则数量: $cleaned_count"
            
            # 3. 排序和去重
            if [[ -s "$cleaned_file" ]]; then
              echo "正在排序和去除重复项..."
              
              # 创建临时文件
              local basic_dedup=$(mktemp)     # 基础去重后的文件
              local wildcard_dedup=$(mktemp)  # 泛域名去重后的文件
              local final_file=$(mktemp)      # 最终文件
              
              # 记录开始时间
              local dedup_start=$SECONDS
              
              # 第1步：基础去重 - 移除完全相同的行
              echo "第1步：执行基础去重（移除完全相同的规则）..."
              LC_ALL=C sort -u -S 50% "$cleaned_file" > "$basic_dedup"
              local cleaned_count=$(wc -l < "$cleaned_file")
              local basic_count=$(wc -l < "$basic_dedup")
              local basic_removed=$((cleaned_count - basic_count))
              echo "基础去重完成：原始 $cleaned_count 条，保留 $basic_count 条，移除 $basic_removed 条"
              
              # 第2步：泛域名去重 - 处理泛域名规则
              echo "第2步：执行泛域名去重（移除被泛域名覆盖的子泛域名）..."
              awk '
              BEGIN {
                total = 0;
                kept = 0;
                removed = 0;
                # 存储所有泛域名
                wildcard_count = 0;
              }
              
              # 处理函数
              {
                line = $0;
                total++;
                
                # 仅处理泛域名（以.开头）
                if (substr(line, 1, 1) == ".") {
                  domain = substr(line, 2);  # 去掉前导点
                  keep = 1;  # 默认保留
                  
                  # 检查是否被已存储的泛域名覆盖
                  for (i = 1; i <= wildcard_count; i++) {
                    parent = wildcards[i];
                    
                    # 检查是否为子域名（比如.a.example.com被.example.com覆盖）
                    if (length(domain) > length(parent) && 
                        substr(domain, length(domain) - length(parent)) == parent && 
                        substr(domain, length(domain) - length(parent) - 1, 1) == ".") {
                      keep = 0;  # 被覆盖，不保留
                      removed++;
                      break;
                    }
                  }
                  
                  if (keep == 1) {
                    # 保留此泛域名
                    wildcard_count++;
                    wildcards[wildcard_count] = domain;
                    kept++;
                    print line;
                  }
                } else {
                  # 非泛域名直接保留
                  print line;
                  kept++;
                }
              }
              
              END {
                print "泛域名总数: " wildcard_count > "/dev/stderr";
                print "处理规则数: " total > "/dev/stderr";
                print "保留规则数: " kept > "/dev/stderr";
                print "移除规则数: " removed > "/dev/stderr";
              }
              ' "$basic_dedup" > "$wildcard_dedup" 2>/tmp/wildcard_stats
              
              # 读取泛域名去重统计
              local wildcard_total=$(grep "泛域名总数" /tmp/wildcard_stats | awk '{print $2}')
              local wildcard_processed=$(grep "处理规则数" /tmp/wildcard_stats | awk '{print $2}')
              local wildcard_kept=$(grep "保留规则数" /tmp/wildcard_stats | awk '{print $2}')
              local wildcard_removed=$(grep "移除规则数" /tmp/wildcard_stats | awk '{print $2}')
              echo "泛域名去重完成：处理 $wildcard_processed 条，保留 $wildcard_kept 条，移除 $wildcard_removed 条"
              
              # 第3步：精确域名去重 - 检查精确域名是否被泛域名覆盖
              echo "第3步：执行精确域名去重（移除被泛域名覆盖的精确域名）..."
              awk '
              BEGIN {
                total = 0;
                kept = 0;
                removed = 0;
                wildcard_count = 0;
              }
              
              # 第一遍：收集所有泛域名
              NR == FNR {
                if (substr($0, 1, 1) == ".") {
                  domain = substr($0, 2);  # 去掉前导点
                  wildcard_count++;
                  wildcards[wildcard_count] = domain;
                }
                next;
              }
              
              # 第二遍：处理所有规则，检查精确域名是否被泛域名覆盖
              {
                line = $0;
                total++;
                
                if (substr(line, 1, 1) == ".") {
                  # 泛域名已经在第二步处理过，直接保留
                  kept++;
                  print line;
                } else {
                  # 精确域名
                  domain = line;
                  keep = 1;  # 默认保留
                  
                  # 检查是否被任何泛域名覆盖
                  for (i = 1; i <= wildcard_count; i++) {
                    parent = wildcards[i];
                    
                    # 检查是否匹配泛域名（如example.com被.example.com覆盖）
                    if (domain == parent || 
                        (length(domain) > length(parent) && 
                         substr(domain, length(domain) - length(parent)) == parent && 
                         substr(domain, length(domain) - length(parent) - 1, 1) == ".")) {
                      keep = 0;  # 被覆盖，不保留
                      removed++;
                      break;
                    }
                  }
                  
                  if (keep == 1) {
                    kept++;
                    print line;
                  }
                }
              }
              
              END {
                print "处理规则数: " total > "/dev/stderr";
                print "保留规则数: " kept > "/dev/stderr";
                print "移除规则数: " removed > "/dev/stderr";
              }
              ' "$wildcard_dedup" "$wildcard_dedup" > "$final_file" 2>/tmp/exact_stats
              
              # 读取精确域名去重统计
              local exact_processed=$(grep "处理规则数" /tmp/exact_stats | awk '{print $2}')
              local exact_kept=$(grep "保留规则数" /tmp/exact_stats | awk '{print $2}')
              local exact_removed=$(grep "移除规则数" /tmp/exact_stats | awk '{print $2}')
              echo "精确域名去重完成：处理 $exact_processed 条，保留 $exact_kept 条，移除 $exact_removed 条"
              
              # 显示总体去重耗时和统计信息
              local dedup_duration=$((SECONDS - dedup_start))
              echo "所有去重完成，总耗时: $dedup_duration 秒"
              echo "去重总统计:"
              echo "- 原始规则数: $cleaned_count"
              echo "- 基础去重后: $basic_count (-$basic_removed)"
              echo "- 泛域名去重后: $wildcard_kept (-$wildcard_removed)"
              echo "- 精确域名去重后: $exact_kept (-$exact_removed)"
              echo "- 总计移除: $((basic_removed + wildcard_removed + exact_removed)) 条规则"
              
              # 清理临时文件以释放内存
              rm -f "$basic_dedup" "$wildcard_dedup" /tmp/wildcard_stats /tmp/exact_stats
              
              # 4. 添加元数据并保存
              echo "正在生成最终规则文件..."
              
              # 创建带有元数据的版本
              local meta_file=$(mktemp)
              
              # 添加元数据
              {
                echo "# 规则来源:"
                for url in $urls; do
                  # 转换为GitHub仓库URL显示
                  repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                  echo "# - https://$repo_url"
                done
                echo ""
                # 添加规则内容
                cat "$final_file"
              } > "$meta_file"
              
              # 5. 检查是否有更改
              local changed=0
              if [ -f "$output_path" ]; then
                # 比较旧文件和新文件(忽略更新时间)
                local old_file=$(mktemp)
                grep -v "^# Update time:" "$output_path" > "$old_file"
                
                if ! cmp -s "$old_file" "$meta_file"; then
                  changed=1  # 文件有变化
                  # 计算增加和删除的规则数量
                  local old_lines=$(grep -v "^#" "$old_file" | wc -l)
                  local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                  local added=$((new_lines > old_lines ? new_lines - old_lines : 0))
                  local removed=$((old_lines > new_lines ? old_lines - new_lines : 0))
                  
                  echo "规则变化统计:"
                  echo "- 原有规则数量: $old_lines"
                  echo "- 新规则数量: $new_lines"
                  echo "- 变化: 新增 $added 条, 移除 $removed 条"
                fi
                rm -f "$old_file"
              else
                changed=1  # 文件不存在,需要创建
                local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                echo "新建规则文件，包含 $new_lines 条规则"
              fi
              
              # 如果有更改，写入新文件
              if [ $changed -eq 1 ]; then
                {
                  echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                  cat "$meta_file"
                } > "$output_path"
                echo "规则已更新"
              else
                echo "规则无变化，无需更新"
              fi
              
              # 清理临时文件
              rm -f "$final_file" "$meta_file"
            else
              echo "警告: 没有找到有效内容，跳过处理"
            fi
            
            # 删除所有临时文件
            rm -f "$merged_file" "$cleaned_file"
            rm -rf "$tmp_dir"
            
            # 显示处理用时
            local duration=$((SECONDS - start_time))
            echo "处理用时: $duration 秒"
          }

          # 主程序 - 控制整个处理流程
          main() {
            echo "===== 网络规则更新程序 ====="
            echo "开始时间: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "============================"
            
            # 初始化数据结构
            declare -A rule_configs  # 存储规则配置
            declare -a rule_files    # 存储规则文件路径
            
            # 获取所有规则配置
            get_rules_config rule_configs rule_files
            
            # 如果没有找到规则，就退出
            if [ ${#rule_configs[@]} -eq 0 ]; then
              echo "没有找到规则配置，程序结束"
              return
            fi
            
            echo "============================"
            echo "开始更新规则"
            echo "============================"
            
            # 记录开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule_name in "${!rule_configs[@]}"; do
              # 分离路径和URL
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule_name]}"
              
              echo "============================"
              # 处理规则
              process_rule "$rule_name" "$output_path" "$urls"
              echo "============================"
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "所有规则处理完成"
            echo "总用时: $((duration / 60))分$((duration % 60))秒"
            
            # 检测是否有变化
            local has_changes=false
            local change_summary=""
            local total_added=0
            local total_removed=0
            
            # 添加文件到Git以检测变化
            git add "${rule_files[@]}" 2>/dev/null || true
            
            # 检查每个文件是否有实质变化
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | 
                 grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                has_changes=true
                local basename=$(basename "$file")
                
                # 计算具体变化的行数
                local added_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^+')
                local removed_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^-')
                
                echo "文件有变化: $file"
                echo "- 详细统计: 新增 $added_lines 条规则, 移除 $removed_lines 条规则"
                
                # 添加到变化摘要
                change_summary="${change_summary}[$basename] +$added_lines/-$removed_lines "
                
                # 累计总变化量
                total_added=$((total_added + added_lines))
                total_removed=$((total_removed + removed_lines))
              fi
            done
            
            # 添加总计到变化摘要
            if [ "$has_changes" = true ]; then
              change_summary="${change_summary}[总计] +$total_added/-$total_removed"
              echo "总变化: 新增 $total_added 条规则, 移除 $total_removed 条规则"
            fi
            
            # 设置输出变量
            if [ "$has_changes" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "change_summary=$change_summary" >> "$GITHUB_OUTPUT"
              echo "规则已更新，即将提交更改"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "规则无变化，无需提交"
              # 还原已暂存的文件
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
            
            echo "============================"
            echo "程序结束"
            echo "============================"
          }

          # 运行主程序
          main
          EOF
          
          chmod +x process_rules.sh
          echo "规则处理脚本已生成，权限设置为可执行"
          echo "脚本生成完成，将执行规则更新..."

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "开始执行规则更新流程..."
          ./process_rules.sh

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "检测到规则变更，准备提交..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # 使用变化摘要作为提交消息
          git commit -m "Auto update rules: ${{ steps.check_changes.outputs.change_summary }}"
          
          # 显示本次提交的统计信息
          echo "提交变更统计:"
          git show --stat --oneline HEAD
          
          echo "推送变更到仓库..."
          git push
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 2  # 保留最近2次运行记录，便于查看历史