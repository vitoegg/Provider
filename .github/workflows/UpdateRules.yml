name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  RULES_CONFIG: |
    rules:
      China:
        path: RuleSet/Direct/China.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
      
      Apple:
        path: RuleSet/Apple/Service.list
        urls:
          - https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
          - https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
      
      Reject:
        path: RuleSet/Extra/Reject.list
        urls:
          - https://ruleset.skk.moe/List/domainset/reject.conf
          - https://ruleset.skk.moe/List/domainset/reject_extra.conf
          - https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # 添加日志输出
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        # 添加日志输出
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        # 添加日志输出
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "::group::创建规则处理脚本"
          echo "创建高效的规则处理脚本..."
          # 创建高效处理脚本
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # ==========================================
          # 规则更新脚本 - 自动分析并优化网络规则集合
          # ==========================================

          # 解析规则配置
          # 参数:
          # $1 - 规则配置文本
          # $2 - 规则配置关联数组的引用 (输出)
          # $3 - 规则文件列表数组的引用 (输出)
          parse_rules_config() {
            local config="$1"
            local -n rule_configs_ref="$2"
            local -n rule_files_ref="$3"

            local rule_name=""
            local path=""
            local urls=""

            while IFS= read -r line; do
              # 跳过空行和注释
              [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

              if [[ "$line" =~ ^[[:space:]]*([^:]+):[[:space:]]*$ ]]; then
                rule_name="${BASH_REMATCH[1]}"
                if [[ "$rule_name" != "rules" ]]; then
                  path=""
                  urls=""
                fi
              elif [[ "$line" =~ ^[[:space:]]*path:[[:space:]]*([^[:space:]]+) ]]; then
                path="${BASH_REMATCH[1]}"
                rule_files_ref+=("$path")
              elif [[ "$line" =~ ^[[:space:]]*urls:[[:space:]]*$ ]]; then
                continue
              elif [[ "$line" =~ ^[[:space:]]*-[[:space:]]*([^[:space:]]+) ]]; then
                urls+="${BASH_REMATCH[1]} "
              elif [[ -n "$rule_name" && -n "$path" && -n "$urls" ]]; then
                rule_configs_ref["$rule_name"]="$path;$urls"
                rule_name=""
              fi
            done <<< "$config"

            # 检查最后一个规则
            if [[ -n "$rule_name" && -n "$path" && -n "$urls" && "$rule_name" != "rules" ]]; then
              rule_configs_ref["$rule_name"]="$path;$urls"
            fi
          }

          # 下载并处理规则
          # 参数:
          # $1 - 规则名称
          # $2 - 输出文件路径
          # $3 - 规则源URL列表（空格分隔）
          process_rule() {
            local rule_name="$1"
            local output_path="$2"
            local urls="$3"
            local output_dir
            output_dir=$(dirname "$output_path")
            
            echo "Processing $rule_name to $output_path..."
            TIMEFORMAT='处理用时: %R 秒'
            time {
              # 创建输出目录
              mkdir -p "$output_dir"
              
              # 临时文件
              local merged_file
              local cleaned_file
              merged_file=$(mktemp)
              cleaned_file=$(mktemp)
              
              # 临时目录和并行下载计数
              local tmp_dir=$(mktemp -d)
              local download_count=0
              local download_pids=()
              
              echo "::group::下载规则源文件"
              echo "→ 并行下载规则文件..."
              # 并行下载规则
              for url in $urls; do
                local tmp_file="${tmp_dir}/download_${download_count}"
                (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && echo "  √ 下载成功: $url" || echo "  × 下载失败: $url") &
                download_pids+=($!)
                download_count=$((download_count + 1))
              done
              
              # 等待所有下载完成
              for pid in "${download_pids[@]}"; do
                wait $pid
              done
              echo "::endgroup::"
              
              echo "::group::合并和清理规则"
              echo "→ 合并和清理规则文件..."
              # 合并下载的文件
              find "$tmp_dir" -type f -name "download_*" -exec cat {} \; > "$merged_file"
              
              # 快速清理文件 (使用更高效的方式)
              LC_ALL=C grep -v "^#\|^$\|^payload:" "$merged_file" > "$cleaned_file" || true
              rm -rf "$tmp_dir"  # 清理临时目录
              
              # 获取数据行数
              local input_count=$(wc -l < "$cleaned_file")
              echo "→ 输入规则数量: $input_count"
              echo "::endgroup::"
              
              # 仅当有内容时才处理域名
              if [[ -s "$cleaned_file" ]]; then
                # 处理格式
                local formatted_file
                formatted_file=$(mktemp)
                
                echo "::group::排序和去重"
                echo "→ 对规则进行排序和去重..."
                # 使用直接的域名处理方法
                LANG=C LC_ALL=C awk '
                BEGIN {
                  # 不区分大小写
                  IGNORECASE = 1
                }
                
                # 预处理：收集所有域名
                {
                  # 清理空白
                  gsub(/^[[:space:]]+|[[:space:]]+$/, "", $0)
                  
                  # 跳过空行
                  if (length($0) == 0) continue
                  
                  # 存储原始域名
                  original_domains[++count] = $0
                  
                  # 标准化域名（移除前导点）
                  domain = $0
                  has_dot = (substr(domain, 1, 1) == ".")
                  
                  # 如果有前导点，记录原始和标准化的映射
                  if (has_dot) {
                    normalized = substr(domain, 2)
                    dot_prefixed[normalized] = domain
                  } else {
                    normalized = domain
                  }
                  
                  # 保存标准化的域名
                  norm_domains[count] = normalized
                  
                  # 记录标准化域名出现的次数
                  if (normalized in domain_count) {
                    domain_count[normalized]++
                  } else {
                    domain_count[normalized] = 1
                  }
                }
                
                END {
                  if (count == 0) exit
                  
                  # 第一步：构建标记数组，并处理带点和不带点的情况
                  for (i = 1; i <= count; i++) {
                    orig = original_domains[i]
                    norm = norm_domains[i]
                    
                    # 如果这个标准化域名有带点和不带点两个版本
                    if (domain_count[norm] > 1 && norm in dot_prefixed) {
                      # 如果当前是不带点版本，标记为删除
                      if (substr(orig, 1, 1) != ".") {
                        to_remove[i] = 1
                      }
                    }
                  }
                  
                  # 第二步：处理父子域名关系
                  # 按标准化域名长度建立索引
                  split("", domain_by_len)
                  for (i = 1; i <= count; i++) {
                    if (!(i in to_remove)) {
                      norm = norm_domains[i]
                      len = length(norm)
                      domain_by_len[len][i] = norm
                    }
                  }
                  
                  # 从短到长处理域名，这样父域名会先于子域名处理
                  lens = asorti(domain_by_len)
                  for (li = 1; li <= length(lens); li++) {
                    len = lens[li]
                    
                    # 遍历当前长度的所有域名
                    for (i in domain_by_len[len]) {
                      if (i in to_remove) continue
                      
                      parent_norm = domain_by_len[len][i]
                      
                      # 检查所有更长的域名是否是这个域名的子域名
                      for (lj = li + 1; lj <= length(lens); lj++) {
                        longer_len = lens[lj]
                        
                        for (j in domain_by_len[longer_len]) {
                          if (j in to_remove) continue
                          
                          child_norm = domain_by_len[longer_len][j]
                          
                          # 检查child是否是parent的子域名
                          if (length(child_norm) > length(parent_norm) + 1 && 
                              substr(child_norm, length(child_norm) - length(parent_norm) + 1) == parent_norm && 
                              substr(child_norm, length(child_norm) - length(parent_norm), 1) == ".") {
                            # j是i的子域名，标记为删除
                            to_remove[j] = 1
                          }
                        }
                      }
                    }
                  }
                  
                  # 输出处理后的域名
                  for (i = 1; i <= count; i++) {
                    if (!(i in to_remove)) {
                      print original_domains[i]
                    }
                  }
                }
                ' "$cleaned_file" > "$formatted_file"
                
                local sorted_count=$(wc -l < "$formatted_file")
                echo "→ 排序后规则数量: $sorted_count"
                echo "::endgroup::"
                
                # 创建最终输出文件
                local final_file
                final_file=$(mktemp)
                
                echo "::group::处理域名关系"
                echo "→ 处理域名关系..."
                # 根据文件类型添加头部
                {
                  if [[ "$output_path" == *.yaml ]]; then
                    echo "payload:"
                  fi
                  
                  # 使用AWK优化处理域名关系 - 性能优化版本
                  LANG=C LC_ALL=C awk -v OFS= '
                  BEGIN {
                    IGNORECASE = 1  # 忽略大小写
                  }
                  {
                    # 提取域名部分
                    domain = $0
                    gsub(/^[[:space:]]+|[[:space:]]+$/, "", domain)
                    
                    # 跳过空行
                    if (length(domain) == 0) continue

                    # 记录所有域名，保持原始顺序
                    all_domains[++count] = domain
                    
                    # 标记是否是带点前缀域名
                    if (substr(domain, 1, 1) == ".") {
                      has_dot[domain] = 1
                      
                      # 获取不带点的版本
                      no_dot_version = substr(domain, 2)
                      
                      # 建立映射：从不带点版本到带点版本
                      no_dot_to_dot[no_dot_version] = domain
                    } else {
                      # 将不带点版本映射到自身
                      no_dot_to_dot[domain] = ""
                    }
                  }
                  END {
                    if (count == 0) exit
                    
                    # 第一步：识别并标记所有应该被移除的域名
                    # 初始化要移除的域名哈希表
                    split("", to_remove)
                    
                    # 首先处理带点前缀和不带点前缀的情况
                    for (i = 1; i <= count; i++) {
                      domain = all_domains[i]
                      
                      # 不处理已标记为移除的域名
                      if (domain in to_remove) continue
                      
                      # 检查是否是带点前缀的域名
                      if (domain in has_dot) {
                        # 获取不带点版本
                        no_dot = substr(domain, 2)
                        
                        # 如果不带点版本在原始域名列表中存在，标记为移除
                        for (j = 1; j <= count; j++) {
                          if (all_domains[j] == no_dot) {
                            to_remove[no_dot] = 1
                            break
                          }
                        }
                      }
                    }
                    
                    # 第二步：处理子域名关系
                    for (i = 1; i <= count; i++) {
                      domain_i = all_domains[i]
                      
                      # 跳过已标记为移除的域名
                      if (domain_i in to_remove) continue
                      
                      # 获取标准化版本（去除前导点）
                      norm_i = domain_i
                      sub(/^\./, "", norm_i)
                      
                      # 检查是否为其他域名的子域名
                      for (j = 1; j <= count; j++) {
                        if (i == j) continue
                        
                        domain_j = all_domains[j]
                        
                        # 跳过已标记为移除的域名
                        if (domain_j in to_remove) continue
                        
                        # 获取标准化版本
                        norm_j = domain_j
                        sub(/^\./, "", norm_j)
                        
                        # 子域名检测: 如果norm_i以norm_j结尾且前面有一个点
                        if (norm_i != norm_j && 
                            length(norm_i) > length(norm_j) + 1 && 
                            substr(norm_i, length(norm_i) - length(norm_j) + 1) == norm_j && 
                            substr(norm_i, length(norm_i) - length(norm_j), 1) == ".") {
                          # domain_i是domain_j的子域名，标记为移除
                          to_remove[domain_i] = 1
                          break
                        }
                      }
                    }
                    
                    # 输出最终保留的域名
                    for (i = 1; i <= count; i++) {
                      domain = all_domains[i]
                      if (!(domain in to_remove)) {
                        print domain
                      }
                    }
                  }
                  ' "$formatted_file"
                } > "$final_file"
                
                # 获取处理后结果数量
                local output_count=$(wc -l < "$final_file")
                echo "→ 处理后规则数量: $output_count (减少了 $((sorted_count - output_count)) 条冗余规则)"
                echo "::endgroup::"
                
                echo "::group::生成最终规则文件"
                # 创建带有元数据的版本
                local meta_file
                meta_file=$(mktemp)
                
                {
                  echo "# Merged from:"
                  for url in $urls; do
                    repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                    echo "# - https://$repo_url"
                  done
                  echo ""
                  cat "$final_file"
                } > "$meta_file"
                
                # 检查是否有更改
                local changed=0
                if [ -f "$output_path" ]; then
                  local old_file
                  old_file=$(mktemp)
                  grep -v "^# Update time:" "$output_path" > "$old_file"
                  
                  if ! cmp -s "$old_file" "$meta_file"; then
                    changed=1
                  fi
                  rm -f "$old_file"
                else
                  changed=1
                fi
                
                # 如果有更改，写入新文件
                if [ $changed -eq 1 ]; then
                  {
                    echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                    cat "$meta_file"
                  } > "$output_path"
                  echo "→ 检测到规则变更，已更新文件"
                else
                  echo "→ 未检测到规则变更"
                fi
                echo "::endgroup::"
                
                # 清理临时文件
                rm -f "$formatted_file" "$final_file" "$meta_file"
              else
                echo "→ 警告: $rule_name 没有有效内容"
              fi
              
              # 清理主要临时文件
              rm -f "$merged_file" "$cleaned_file"
            }
          }

          # 主函数 - 协调整个规则更新过程
          # 参数:
          # $1 - 完整的规则配置
          main() {
            local config="$1"
            
            # 初始化数据结构
            declare -A rule_configs
            declare -a rule_files
            
            # 解析配置
            parse_rules_config "$config" rule_configs rule_files
            
            echo "==============================="
            echo "开始更新规则文件 ($(date '+%Y-%m-%d %H:%M:%S'))"
            echo "==============================="
            
            # 记录总体开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule in "${!rule_configs[@]}"; do
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule]}"
              echo "==============================="
              echo "::group::处理规则: $rule"
              process_rule "$rule" "$output_path" "$urls"
              echo "::endgroup::"
              echo "==============================="
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "::group::处理总结"
            echo "所有规则处理完成，总耗时: $((duration / 60))分$((duration % 60))秒"
            
            # 性能检查
            if [ $duration -gt 180 ]; then
              echo "警告: 处理时间超过3分钟，建议进一步优化性能"
            elif [ $duration -lt 30 ]; then
              echo "性能良好: 处理时间少于30秒"
            else
              echo "性能正常: 处理时间在目标范围内"
            fi
            echo "::endgroup::"
            
            # 检查Git变更
            local changes_detected=false
            
            git add "${rule_files[@]}" 2>/dev/null || true
            
            echo "::group::检查文件变更"
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                changes_detected=true
                echo "检测到变更: $file"
              fi
            done
            echo "::endgroup::"
            
            # 输出结果
            if [ "$changes_detected" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "检测到规则文件变更"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "未检测到规则文件变更"
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
          }

          # 运行主函数
          main "$1"
          EOF
          
          chmod +x process_rules.sh
          echo "脚本创建完成"
          echo "::endgroup::"

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "::group::开始更新规则集"
          echo "开始规则处理流程..."
          ./process_rules.sh "${{ env.RULES_CONFIG }}"
          echo "::endgroup::"

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "::group::提交变更"
          echo "检测到规则变更，提交到仓库..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Auto update rules by action bot"
          
          # 显示本次提交的统计信息但不显示具体内容
          echo "本次更新统计:"
          git show --stat --oneline HEAD
          
          # 推送更改
          echo "推送更改到远程仓库..."
          git push
          echo "::endgroup::"
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 6  # 保留最近6次运行记录，便于查看历史
