name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  # 规则配置 - 简单格式
  # 格式: RULE_名称_PATH=输出路径 和 RULE_名称_URLS=URL1 URL2 URL3
  
  # China规则
  RULE_CHINA_PATH: RuleSet/Direct/China.list
  RULE_CHINA_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
  
  # Apple规则
  RULE_APPLE_PATH: RuleSet/Apple/Service.list
  RULE_APPLE_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
  
  # Reject规则
  RULE_REJECT_PATH: RuleSet/Extra/Reject.list
  RULE_REJECT_URLS: >-
    https://ruleset.skk.moe/List/domainset/reject.conf
    https://ruleset.skk.moe/List/domainset/reject_extra.conf
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "开始生成规则处理脚本..."
          echo "脚本功能: 下载规则、清理规则、合并规则、统计变化"
          echo "脚本结构: 3个主要函数 - 配置读取、规则处理、主程序"
          
          cat > process_rules.sh << EOF
          #!/bin/bash
          set -eo pipefail

          # =====================================
          # 网络规则更新脚本
          # 功能: 下载、清理、去掉重复的规则
          # =====================================

          # 第一步: 从环境变量获取规则配置信息
          get_rules_config() {
            # 接收参数
            local -n rule_list="$1"   # 存放规则配置的数组
            local -n file_list="$2"   # 存放文件路径的数组
            
            echo "正在查找规则配置..."
            
            # 查找所有规则变量 (格式: RULE_规则名_PATH)
            for var in $(env | grep "^RULE_.*_PATH=" | cut -d= -f1); do
              # 从变量名中提取规则名称
              local rule_name=${var%_PATH}   # 去掉_PATH后缀
              rule_name=${rule_name#RULE_}   # 去掉RULE_前缀
              
              # 获取对应的URL变量名 (格式: RULE_规则名_URLS)
              local url_var="RULE_${rule_name}_URLS"
              
              # 检查URL变量是否存在
              if [[ -v $url_var ]]; then
                # 获取路径和URL值
                local path=${!var}             # 获取路径变量的值
                local urls=${!url_var}         # 获取URL变量的值
                
                # 显示找到的规则信息
                echo "找到规则: $rule_name"
                echo "- 保存路径: $path"
                echo "- 下载地址数量: $(echo "$urls" | wc -w)"
                
                # 保存规则信息到数组中
                rule_list["$rule_name"]="$path;$urls"  # 路径和URL用分号分隔
                file_list+=("$path")                   # 添加到文件列表
              fi
            done
            
            # 显示找到的规则总数
            echo "共找到 ${#rule_list[@]} 个规则"
          }

          # 第二步: 处理单个规则
          process_rule() {
            # 接收参数
            local rule_name="$1"      # 规则名称
            local output_path="$2"    # 输出文件路径
            local urls="$3"           # URL列表
            
            # 创建输出目录(如果不存在)
            local output_dir=$(dirname "$output_path")
            mkdir -p "$output_dir"
            
            echo "开始处理规则: $rule_name"
            echo "保存位置: $output_path"
            
            # 计时
            local start_time=$SECONDS
            
            # 1. 下载规则文件
            echo "正在下载规则..."
            
            # 创建临时文件
            local merged_file=$(mktemp)  # 合并后的文件
            local cleaned_file=$(mktemp) # 清理后的文件
            local tmp_dir=$(mktemp -d)   # 临时下载目录
            
            # 并行下载所有URL
            local download_count=0
            local download_pids=()
            
            for url in $urls; do
              # 为每个URL创建临时文件
              local tmp_file="${tmp_dir}/download_${download_count}"
              
              # 后台下载并显示结果
              (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && 
               echo "下载成功: $url" || 
               echo "下载失败: $url") &
              
              # 保存进程ID和计数
              download_pids+=($!)
              download_count=$((download_count + 1))
            done
            
            # 等待所有下载完成
            for pid in "${download_pids[@]}"; do
              wait $pid
            done
            
            # 2. 合并和清理规则
            echo "正在合并和清理规则..."
            
            # 合并所有下载的文件
            cat "${tmp_dir}"/download_* > "$merged_file"
            
            # 清理规则:
            # - 删除注释和空行
            # - 删除行首尾空格
            sed -e 's/[[:space:]]*[#;\/\/].*$//' \
                -e 's/^[[:space:]]*//;s/[[:space:]]*$//' \
                -e '/^$/d' \
                -e '/^[[:space:]]*[#;\/\/]/d' \
                -e '/^payload:/d' \
                -e '/^[[:space:]]*\/\*/d;/\*\//d' \
                "$merged_file" > "$cleaned_file"
            
            # 统计清理后的规则数量
            local cleaned_count=$(wc -l < "$cleaned_file")
            echo "清理后的规则数量: $cleaned_count"
            
            # 3. 排序和去重
            if [[ -s "$cleaned_file" ]]; then
              echo "正在排序和去除重复项..."
              
              # 创建临时文件
              local domains_file=$(mktemp)
              local wildcard_file=$(mktemp)
              local final_file=$(mktemp)
              
              # 先排序(不去重)，确保数据有序
              sort "$cleaned_file" > "$domains_file"
              
              # 分离出泛域名和普通域名
              echo "正在处理泛域名和普通域名..."
              grep "^\\." "$domains_file" > "$wildcard_file" || true
              
              # 创建泛域名索引，用于高效查找
              echo "正在创建泛域名索引..."
              local wildcard_count=$(wc -l < "$wildcard_file")
              echo "- 发现 $wildcard_count 个泛域名"
              
              # 去重处理：删除被泛域名覆盖的精确域名
              echo "正在根据泛域名规则清理冗余域名..."
              
              if [[ $wildcard_count -gt 0 ]]; then
                # 超高性能泛域名处理方案
                echo "使用超高性能方案处理大规模规则..."
                
                # 创建临时目录存放中间文件
                local tmp_process_dir=$(mktemp -d)
                local normalized_file="${tmp_process_dir}/normalized.txt"
                local wildcard_file_sorted="${tmp_process_dir}/wildcards_sorted.txt"
                local result_file="${tmp_process_dir}/result.txt"
                
                # 1. 预处理 - 将泛域名和普通域名标准化存储
                echo "预处理域名数据..."
                # 并行处理，使用管道和多进程
                {
                  # 提取泛域名，标记类型，去掉前导点
                  grep "^\\." "$domains_file" | sed 's/^\./W /' > "${tmp_process_dir}/wildcards.txt" &
                  
                  # 提取普通域名，标记类型
                  grep -v "^\\." "$domains_file" | sed 's/^/E /' > "${tmp_process_dir}/exact.txt" &
                  
                  # 等待两个进程完成
                  wait
                  
                  # 合并结果
                  cat "${tmp_process_dir}/wildcards.txt" "${tmp_process_dir}/exact.txt" > "$normalized_file"
                  
                  # 泛域名单独排序 (按长度，用于泛域名间去重)
                  awk -F' ' '$1=="W" {print length($2) "\t" $0}' "$normalized_file" | 
                  sort -n | cut -f2- > "$wildcard_file_sorted"
                } &> /dev/null
                
                # 获取泛域名数量
                local wildcard_norm_count=$(wc -l < "$wildcard_file_sorted")
                echo "处理 $wildcard_norm_count 个泛域名..."
                
                # 2. 泛域名去重 - 使用C语言级别的高效算法
                # 使用printf方式生成C代码文件，避免heredoc问题
                printf '%s\n' \
                '#include <stdio.h>' \
                '#include <stdlib.h>' \
                '#include <string.h>' \
                '' \
                '#define MAX_DOMAIN_LEN 256' \
                '' \
                '// 泛域名结构' \
                'typedef struct {' \
                '    char domain[MAX_DOMAIN_LEN];' \
                '    int keep;  // 1表示保留，0表示删除' \
                '} WildcardDomain;' \
                '' \
                '// 检查domain_b是否是domain_a的子域名' \
                'int is_subdomain(const char *domain_a, const char *domain_b) {' \
                '    int len_a = strlen(domain_a);' \
                '    int len_b = strlen(domain_b);' \
                '    ' \
                '    // 完全相同' \
                '    if (strcmp(domain_a, domain_b) == 0) return 1;' \
                '    ' \
                '    // b更长，检查b是否以.a结尾' \
                '    if (len_b > len_a) {' \
                '        // 检查b是否以.a结尾' \
                '        if (domain_b[len_b - len_a - 1] == '\''.'\'' && ' \
                '            strcmp(domain_b + (len_b - len_a), domain_a) == 0) {' \
                '            return 1;' \
                '        }' \
                '    }' \
                '    ' \
                '    return 0;' \
                '}' \
                '' \
                'int main() {' \
                '    FILE *in = fopen("wildcards_sorted.txt", "r");' \
                '    FILE *out = fopen("wildcards_filtered.txt", "w");' \
                '    ' \
                '    if (!in || !out) {' \
                '        fprintf(stderr, "文件打开失败\\n");' \
                '        return 1;' \
                '    }' \
                '    ' \
                '    char line[MAX_DOMAIN_LEN];' \
                '    char type[2];' \
                '    char domain[MAX_DOMAIN_LEN];' \
                '    ' \
                '    // 第一阶段：读取所有泛域名并标记' \
                '    WildcardDomain *wildcards = NULL;' \
                '    int wildcard_count = 0;' \
                '    int capacity = 1024;' \
                '    ' \
                '    wildcards = (WildcardDomain*)malloc(capacity * sizeof(WildcardDomain));' \
                '    ' \
                '    // 读取所有泛域名' \
                '    while (fgets(line, MAX_DOMAIN_LEN, in)) {' \
                '        if (sscanf(line, "%1s %255s", type, domain) != 2) continue;' \
                '        ' \
                '        if (wildcard_count >= capacity) {' \
                '            capacity *= 2;' \
                '            wildcards = (WildcardDomain*)realloc(wildcards, capacity * sizeof(WildcardDomain));' \
                '        }' \
                '        ' \
                '        strncpy(wildcards[wildcard_count].domain, domain, MAX_DOMAIN_LEN - 1);' \
                '        wildcards[wildcard_count].keep = 1;' \
                '        wildcard_count++;' \
                '    }' \
                '    ' \
                '    // 泛域名去重' \
                '    int removed = 0;' \
                '    ' \
                '    // 由于已按长度排序，短域名在前，只需检查每个域名是否被前面的域名覆盖' \
                '    for (int i = 1; i < wildcard_count; i++) {' \
                '        if (!wildcards[i].keep) continue;' \
                '        ' \
                '        for (int j = 0; j < i; j++) {' \
                '            if (!wildcards[j].keep) continue;' \
                '            ' \
                '            if (is_subdomain(wildcards[j].domain, wildcards[i].domain)) {' \
                '                wildcards[i].keep = 0;' \
                '                removed++;' \
                '                break;' \
                '            }' \
                '        }' \
                '    }' \
                '    ' \
                '    // 输出结果' \
                '    for (int i = 0; i < wildcard_count; i++) {' \
                '        if (wildcards[i].keep) {' \
                '            fprintf(out, "W %s\\n", wildcards[i].domain);' \
                '        }' \
                '    }' \
                '    ' \
                '    fprintf(stderr, "- 泛域名去重: 删除了 %d 个冗余泛域名\\n", removed);' \
                '    ' \
                '    fclose(in);' \
                '    fclose(out);' \
                '    free(wildcards);' \
                '    ' \
                '    return 0;' \
                '}' > "${tmp_process_dir}/process.c"
                
                # 编译并运行C程序
                echo "编译并运行高性能处理程序..."
                (cd "${tmp_process_dir}" && gcc -O3 -o process process.c && ./process)
                
                # 获取泛域名去重后的数量
                local wildcard_filtered_count=$(wc -l < "${tmp_process_dir}/wildcards_filtered.txt")
                echo "- 泛域名去重后剩余: $wildcard_filtered_count 个泛域名"
                
                # 3. 精确域名过滤 - 使用更高效率的C程序
                # 使用printf方式生成C代码文件，避免heredoc问题
                printf '%s\n' \
                '#include <stdio.h>' \
                '#include <stdlib.h>' \
                '#include <string.h>' \
                '' \
                '#define MAX_DOMAIN_LEN 256' \
                '#define HASH_SIZE 1000003' \
                '' \
                '// 简单的哈希表实现' \
                'typedef struct HashNode {' \
                '    char key[MAX_DOMAIN_LEN];' \
                '    struct HashNode *next;' \
                '} HashNode;' \
                '' \
                'HashNode *hash_table[HASH_SIZE] = {NULL};' \
                '' \
                '// 简单哈希函数' \
                'unsigned int hash(const char *str) {' \
                '    unsigned int h = 0;' \
                '    while (*str) {' \
                '        h = h * 31 + (*str++);' \
                '    }' \
                '    return h % HASH_SIZE;' \
                '}' \
                '' \
                '// 添加到哈希表' \
                'void hash_add(const char *key) {' \
                '    unsigned int h = hash(key);' \
                '    ' \
                '    HashNode *node = (HashNode*)malloc(sizeof(HashNode));' \
                '    strncpy(node->key, key, MAX_DOMAIN_LEN - 1);' \
                '    node->key[MAX_DOMAIN_LEN - 1] = '\'\\0\'';' \
                '    node->next = hash_table[h];' \
                '    hash_table[h] = node;' \
                '}' \
                '' \
                '// 检查是否在哈希表中' \
                'int hash_contains(const char *key) {' \
                '    unsigned int h = hash(key);' \
                '    ' \
                '    HashNode *node = hash_table[h];' \
                '    while (node) {' \
                '        if (strcmp(node->key, key) == 0) {' \
                '            return 1;' \
                '        }' \
                '        node = node->next;' \
                '    }' \
                '    return 0;' \
                '}' \
                '' \
                'int main() {' \
                '    FILE *wildcards = fopen("wildcards_filtered.txt", "r");' \
                '    FILE *normalized = fopen("normalized.txt", "r");' \
                '    FILE *result = fopen("result.txt", "w");' \
                '    ' \
                '    if (!wildcards || !normalized || !result) {' \
                '        fprintf(stderr, "文件打开失败\\n");' \
                '        return 1;' \
                '    }' \
                '    ' \
                '    char line[MAX_DOMAIN_LEN];' \
                '    char type[2];' \
                '    char domain[MAX_DOMAIN_LEN];' \
                '    ' \
                '    // 1. 读取所有泛域名并建立哈希表' \
                '    while (fgets(line, MAX_DOMAIN_LEN, wildcards)) {' \
                '        if (sscanf(line, "%1s %255s", type, domain) != 2) continue;' \
                '        ' \
                '        // 添加到哈希表' \
                '        hash_add(domain);' \
                '        ' \
                '        // 同时输出泛域名（需要保留）' \
                '        fprintf(result, ".%s\\n", domain);' \
                '    }' \
                '    fclose(wildcards);' \
                '    ' \
                '    // 2. 处理所有域名' \
                '    int kept = 0;' \
                '    int filtered = 0;' \
                '    int total = 0;' \
                '    ' \
                '    while (fgets(line, MAX_DOMAIN_LEN, normalized)) {' \
                '        if (sscanf(line, "%1s %255s", type, domain) != 2) continue;' \
                '        ' \
                '        // 跳过泛域名，已经处理过' \
                '        if (type[0] == '\'W\'') continue;' \
                '        ' \
                '        total++;' \
                '        ' \
                '        // 检查是否被任何泛域名覆盖' \
                '        int keep = 1;' \
                '        char temp[MAX_DOMAIN_LEN];' \
                '        strncpy(temp, domain, MAX_DOMAIN_LEN);' \
                '        ' \
                '        while (temp[0]) {' \
                '            if (hash_contains(temp)) {' \
                '                keep = 0;' \
                '                filtered++;' \
                '                break;' \
                '            }' \
                '            ' \
                '            // 获取父域名' \
                '            char *dot = strchr(temp, '\''.\'');' \
                '            if (!dot) break;' \
                '            ' \
                '            // 移到下一个部分' \
                '            memmove(temp, dot + 1, strlen(dot));' \
                '        }' \
                '        ' \
                '        if (keep) {' \
                '            fprintf(result, "%s\\n", domain);' \
                '            kept++;' \
                '        }' \
                '        ' \
                '        // 打印进度' \
                '        if (total % 10000 == 0) {' \
                '            fprintf(stderr, "处理进度: %d (保留: %d, 过滤: %d)\\n", total, kept, filtered);' \
                '        }' \
                '    }' \
                '    ' \
                '    fprintf(stderr, "- 精确域名过滤完成: 保留 %d 个, 过滤: %d 个\\n", kept, filtered);' \
                '    ' \
                '    fclose(normalized);' \
                '    fclose(result);' \
                '    ' \
                '    // 清理哈希表' \
                '    for (int i = 0; i < HASH_SIZE; i++) {' \
                '        HashNode *node = hash_table[i];' \
                '        while (node) {' \
                '            HashNode *next = node->next;' \
                '            free(node);' \
                '            node = next;' \
                '        }' \
                '    }' \
                '    ' \
                '    return 0;' \
                '}' > "${tmp_process_dir}/filter_exact.c"
                
                # 编译并运行过滤程序
                echo "编译并运行高性能域名过滤程序..."
                (cd "${tmp_process_dir}" && gcc -O3 -o filter_exact filter_exact.c && ./filter_exact)
                
                # 4. 最后处理 - 复制结果
                cat "${tmp_process_dir}/result.txt" > "$final_file"
                
                # 统计结果
                local final_count=$(wc -l < "$final_file")
                local removed_count=$((cleaned_count - final_count))
                echo "去重后的规则数量: $final_count (减少了 $removed_count 个重复项)"
                
                # 清理临时目录
                rm -rf "${tmp_process_dir}"
              else
                # 如果没有泛域名，只进行普通去重
                sort -u "$domains_file" > "$final_file"
              fi
              
              # 4. 添加元数据并保存
              echo "正在生成最终规则文件..."
              
              # 创建带有元数据的版本
              local meta_file=$(mktemp)
              
              # 添加元数据
              {
                echo "# 规则来源:"
                for url in $urls; do
                  # 转换为GitHub仓库URL显示
                  repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                  echo "# - https://$repo_url"
                done
                echo ""
                # 添加规则内容
                cat "$final_file"
              } > "$meta_file"
              
              # 5. 检查是否有更改
              local changed=0
              if [ -f "$output_path" ]; then
                # 比较旧文件和新文件(忽略更新时间)
                local old_file=$(mktemp)
                grep -v "^# Update time:" "$output_path" > "$old_file"
                
                if ! cmp -s "$old_file" "$meta_file"; then
                  changed=1  # 文件有变化
                  # 计算增加和删除的规则数量
                  local old_lines=$(grep -v "^#" "$old_file" | wc -l)
                  local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                  local added=$((new_lines > old_lines ? new_lines - old_lines : 0))
                  local removed=$((old_lines > new_lines ? old_lines - new_lines : 0))
                  
                  echo "规则变化统计:"
                  echo "- 原有规则数量: $old_lines"
                  echo "- 新规则数量: $new_lines"
                  echo "- 变化: 新增 $added 条, 移除 $removed 条"
                fi
                rm -f "$old_file"
              else
                changed=1  # 文件不存在,需要创建
                local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                echo "新建规则文件，包含 $new_lines 条规则"
              fi
              
              # 如果有更改，写入新文件
              if [ $changed -eq 1 ]; then
                {
                  echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                  cat "$meta_file"
                } > "$output_path"
                echo "规则已更新"
              else
                echo "规则无变化，无需更新"
              fi
              
              # 清理临时文件
              rm -f "$final_file" "$meta_file"
            else
              echo "警告: 没有找到有效内容，跳过处理"
            fi
            
            # 删除所有临时文件
            rm -f "$merged_file" "$cleaned_file"
            rm -rf "$tmp_dir"
            
            # 显示处理用时
            local duration=$((SECONDS - start_time))
            echo "处理用时: $duration 秒"
          }

          # 主程序 - 控制整个处理流程
          main() {
            echo "===== 网络规则更新程序 ====="
            echo "开始时间: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "============================"
            
            # 初始化数据结构
            declare -A rule_configs  # 存储规则配置
            declare -a rule_files    # 存储规则文件路径
            
            # 获取所有规则配置
            get_rules_config rule_configs rule_files
            
            # 如果没有找到规则，就退出
            if [ ${#rule_configs[@]} -eq 0 ]; then
              echo "没有找到规则配置，程序结束"
              return
            fi
            
            echo "============================"
            echo "开始更新规则"
            echo "============================"
            
            # 记录开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule_name in "${!rule_configs[@]}"; do
              # 分离路径和URL
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule_name]}"
              
              echo "============================"
              # 处理规则
              process_rule "$rule_name" "$output_path" "$urls"
              echo "============================"
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "所有规则处理完成"
            echo "总用时: $((duration / 60))分$((duration % 60))秒"
            
            # 检测是否有变化
            local has_changes=false
            local change_summary=""
            local total_added=0
            local total_removed=0
            
            # 添加文件到Git以检测变化
            git add "${rule_files[@]}" 2>/dev/null || true
            
            # 检查每个文件是否有实质变化
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | 
                 grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                has_changes=true
                local basename=$(basename "$file")
                
                # 计算具体变化的行数
                local added_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^+')
                local removed_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^-')
                
                echo "文件有变化: $file"
                echo "- 详细统计: 新增 $added_lines 条规则, 移除 $removed_lines 条规则"
                
                # 添加到变化摘要
                change_summary="${change_summary}[$basename] +$added_lines/-$removed_lines "
                
                # 累计总变化量
                total_added=$((total_added + added_lines))
                total_removed=$((total_removed + removed_lines))
              fi
            done
            
            # 添加总计到变化摘要
            if [ "$has_changes" = true ]; then
              change_summary="${change_summary}[总计] +$total_added/-$total_removed"
              echo "总变化: 新增 $total_added 条规则, 移除 $total_removed 条规则"
            fi
            
            # 设置输出变量
            if [ "$has_changes" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "change_summary=$change_summary" >> "$GITHUB_OUTPUT"
              echo "规则已更新，即将提交更改"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "规则无变化，无需提交"
              # 还原已暂存的文件
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
            
            echo "============================"
            echo "程序结束"
            echo "============================"
          }

          # 运行主程序
          main
          EOF
          
          chmod +x process_rules.sh
          echo "规则处理脚本已生成，权限设置为可执行"
          echo "脚本生成完成，将执行规则更新..."

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "开始执行规则更新流程..."
          ./process_rules.sh

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "检测到规则变更，准备提交..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # 使用变化摘要作为提交消息
          git commit -m "Auto update rules: ${{ steps.check_changes.outputs.change_summary }}"
          
          # 显示本次提交的统计信息
          echo "提交变更统计:"
          git show --stat --oneline HEAD
          
          echo "推送变更到仓库..."
          git push
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 2  # 保留最近2次运行记录，便于查看历史
