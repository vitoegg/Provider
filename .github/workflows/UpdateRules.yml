name: Update Rules

on:
  schedule:
    - cron: '0 */3 * * *'  # 每3小时执行一次
  workflow_dispatch:

env:
  # 规则配置 - 简单格式
  # 格式: RULE_名称_PATH=输出路径 和 RULE_名称_URLS=URL1 URL2 URL3
  
  # China规则
  RULE_CHINA_PATH: RuleSet/Direct/China.list
  RULE_CHINA_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/ChinaMax/ChinaMax_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/direct.txt
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Direct/LocalNet.list
  
  # Apple规则
  RULE_APPLE_PATH: RuleSet/Apple/Service.list
  RULE_APPLE_URLS: >-
    https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/rule/Surge/Apple/Apple_Domain.list
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/apple.txt
    https://raw.githubusercontent.com/Loyalsoldier/surge-rules/release/icloud.txt
  
  # Reject规则
  RULE_REJECT_PATH: RuleSet/Extra/Reject.list
  RULE_REJECT_URLS: >-
    https://ruleset.skk.moe/List/domainset/reject.conf
    https://ruleset.skk.moe/List/domainset/reject_extra.conf
    https://raw.githubusercontent.com/vitoegg/Provider/master/RuleSet/Extra/Privacy.list

jobs:
  update-rules:
    # 使用 Ubuntu 最新版本作为运行环境
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        id: checkout
        with:
          fetch-depth: 1  # 仅获取最近一次提交，加速检出过程

      - name: Setup Timezone
        run: sudo timedatectl set-timezone "Asia/Shanghai"
        id: timezone

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
        id: python

      - name: Create Processing Script
        id: create_script
        run: |
          echo "开始生成规则处理脚本..."
          echo "脚本功能: 下载规则、清理规则、合并规则、统计变化"
          echo "脚本结构: 3个主要函数 - 配置读取、规则处理、主程序"
          
          cat > process_rules.sh << 'EOF'
          #!/bin/bash
          set -eo pipefail

          # =====================================
          # 网络规则更新脚本
          # 功能: 下载、清理、去掉重复的规则
          # =====================================

          # 第一步: 从环境变量获取规则配置信息
          get_rules_config() {
            # 接收参数
            local -n rule_list="$1"   # 存放规则配置的数组
            local -n file_list="$2"   # 存放文件路径的数组
            
            echo "正在查找规则配置..."
            
            # 查找所有规则变量 (格式: RULE_规则名_PATH)
            for var in $(env | grep "^RULE_.*_PATH=" | cut -d= -f1); do
              # 从变量名中提取规则名称
              local rule_name=${var%_PATH}   # 去掉_PATH后缀
              rule_name=${rule_name#RULE_}   # 去掉RULE_前缀
              
              # 获取对应的URL变量名 (格式: RULE_规则名_URLS)
              local url_var="RULE_${rule_name}_URLS"
              
              # 检查URL变量是否存在
              if [[ -v $url_var ]]; then
                # 获取路径和URL值
                local path=${!var}             # 获取路径变量的值
                local urls=${!url_var}         # 获取URL变量的值
                
                # 显示找到的规则信息
                echo "找到规则: $rule_name"
                echo "- 保存路径: $path"
                echo "- 下载地址数量: $(echo "$urls" | wc -w)"
                
                # 保存规则信息到数组中
                rule_list["$rule_name"]="$path;$urls"  # 路径和URL用分号分隔
                file_list+=("$path")                   # 添加到文件列表
              fi
            done
            
            # 显示找到的规则总数
            echo "共找到 ${#rule_list[@]} 个规则"
          }

          # 第二步: 处理单个规则
          process_rule() {
            # 接收参数
            local rule_name="$1"      # 规则名称
            local output_path="$2"    # 输出文件路径
            local urls="$3"           # URL列表
            
            # 创建输出目录(如果不存在)
            local output_dir=$(dirname "$output_path")
            mkdir -p "$output_dir"
            
            echo "开始处理规则: $rule_name"
            echo "保存位置: $output_path"
            
            # 计时
            local start_time=$SECONDS
            
            # 1. 下载规则文件
            echo "正在下载规则..."
            
            # 创建临时文件
            local merged_file=$(mktemp)  # 合并后的文件
            local cleaned_file=$(mktemp) # 清理后的文件
            local tmp_dir=$(mktemp -d)   # 临时下载目录
            
            # 并行下载所有URL
            local download_count=0
            local download_pids=()
            
            for url in $urls; do
              # 为每个URL创建临时文件
              local tmp_file="${tmp_dir}/download_${download_count}"
              
              # 后台下载并显示结果
              (curl -sL --fail --connect-timeout 10 --max-time 30 "$url" > "$tmp_file" && 
               echo "下载成功: $url" || 
               echo "下载失败: $url") &
              
              # 保存进程ID和计数
              download_pids+=($!)
              download_count=$((download_count + 1))
            done
            
            # 等待所有下载完成
            for pid in "${download_pids[@]}"; do
              wait $pid
            done
            
            # 2. 合并和清理规则
            echo "正在合并和清理规则..."
            
            # 合并所有下载的文件
            cat "${tmp_dir}"/download_* > "$merged_file"
            
            # 清理规则:
            # - 删除注释和空行
            # - 删除行首尾空格
            sed -e 's/[[:space:]]*[#;\/\/].*$//' \
                -e 's/^[[:space:]]*//;s/[[:space:]]*$//' \
                -e '/^$/d' \
                -e '/^[[:space:]]*[#;\/\/]/d' \
                -e '/^payload:/d' \
                -e '/^[[:space:]]*\/\*/d;/\*\//d' \
                "$merged_file" > "$cleaned_file"
            
            # 统计清理后的规则数量
            local cleaned_count=$(wc -l < "$cleaned_file")
            echo "清理后的规则数量: $cleaned_count"
            
            # 3. 排序和去重
            if [[ -s "$cleaned_file" ]]; then
              echo "正在排序和去除重复项..."
              
              # 创建临时文件
              local deduped_file=$(mktemp)
              local domains_file=$(mktemp)
              local wildcard_file=$(mktemp)
              local exact_file=$(mktemp)
              local final_file=$(mktemp)
              
              # 先进行基础去重，减少数据量
              echo "进行基础去重..."
              sort -u "$cleaned_file" > "$deduped_file"
              local basic_dedup_count=$(wc -l < "$deduped_file")
              local initial_removed=$((cleaned_count - basic_dedup_count))
              echo "基础去重: 从 $cleaned_count 减少到 $basic_dedup_count (减少了 $initial_removed 个完全重复项)"
              
              # 对去重后的数据进行分类和排序
              echo "分离和排序域名..."
              sort "$deduped_file" > "$domains_file"
              
              # 提取泛域名（以.开头的域名）和普通域名
              echo "分离泛域名和普通域名..."
              grep "^\\." "$domains_file" > "$wildcard_file" || true
              grep -v "^\\." "$domains_file" > "$exact_file" || true
              
              # 获取泛域名和普通域名的数量
              local wildcard_count=$(wc -l < "$wildcard_file")
              local exact_count=$(wc -l < "$exact_file")
              echo "共发现 $wildcard_count 个泛域名和 $exact_count 个普通域名"
              
              if [[ $wildcard_count -gt 0 && $exact_count -gt 0 ]]; then
                echo "执行泛域名规则去重..."
                
                # 泛域名去重 - 使用两步过滤方法
                # 1. 处理泛域名之间的冗余（如果有.example.com，则不需要.sub.example.com）
                echo "处理泛域名之间的冗余关系..."
                
                local deduped_wildcards=$(mktemp)
                
                # 先按域名长度排序（短的在前，这些通常是父域名）
                awk '{print length($0) "\t" $0}' "$wildcard_file" | sort -n | cut -f2- > "$deduped_wildcards"
                
                # 保留不冗余的泛域名
                local final_wildcards=$(mktemp)
                if [[ -s "$deduped_wildcards" ]]; then
                  # 一行行处理，检查每个域名是否是前面某个域名的子域名
                  awk '
                  BEGIN {
                    count = 0;
                  }
                  {
                    current = substr($0, 2);  # 去掉前导点
                    keep = 1;
                    
                    # 检查是否是前面已保留域名的子域名
                    for (i = 0; i < count; i++) {
                      parent = domains[i];
                      if (current == parent) {
                        # 完全相同，这不应该发生，但为了安全起见
                        keep = 0;
                        break;
                      }
                      else if (index(current, "." parent) > 0 && 
                              substr(current, length(current) - length(parent)) == parent) {
                        # 当前域名是已知父域名的子域名
                        keep = 0;
                        break;
                      }
                    }
                    
                    if (keep) {
                      print $0;
                      domains[count++] = current;
                    }
                  }
                  ' "$deduped_wildcards" > "$final_wildcards"
                else
                  cp "$deduped_wildcards" "$final_wildcards"
                fi
                
                local wildcards_filtered_count=$(wc -l < "$final_wildcards")
                local wildcards_removed=$((wildcard_count - wildcards_filtered_count))
                echo "- 泛域名去重: 从 $wildcard_count 减少到 $wildcards_filtered_count (减少了 $wildcards_removed 个)"
                
                # 2. 过滤被泛域名覆盖的精确域名
                echo "过滤被泛域名覆盖的精确域名..."
                
                # 预处理泛域名文件：去掉前导点，方便匹配
                local wildcards_processed=$(mktemp)
                sed 's/^\.//' "$final_wildcards" > "$wildcards_processed"
                
                # 使用awk过滤精确域名
                local final_exact=$(mktemp)
                
                awk -v wildcard_file="$wildcards_processed" '
                BEGIN {
                  # 读取所有泛域名到数组
                  while ((getline wcard < wildcard_file) > 0) {
                    wildcards[wcard] = 1;
                  }
                  close(wildcard_file);
                  kept = 0;
                  filtered = 0;
                }
                
                {
                  domain = $0;
                  keep = 1;
                  
                  # 检查是否与任何泛域名匹配
                  # 先检查整个域名是否匹配
                  if (wildcards[domain] == 1) {
                    keep = 0;
                    filtered++;
                  } else {
                    # 检查父域名是否匹配
                    split_pos = index(domain, ".");
                    if (split_pos > 0) {
                      parent = substr(domain, split_pos + 1);
                      if (wildcards[parent] == 1) {
                        keep = 0;
                        filtered++;
                      } else {
                        # 进一步检查上一级父域名
                        while (split_pos = index(parent, ".")) {
                          parent = substr(parent, split_pos + 1);
                          if (wildcards[parent] == 1) {
                            keep = 0;
                            filtered++;
                            break;
                          }
                        }
                      }
                    }
                  }
                  
                  if (keep) {
                    print domain;
                    kept++;
                  }
                  
                  # 打印进度（每处理10万行）
                  total = kept + filtered;
                  if (total % 100000 == 0) {
                    printf "处理进度: %d\n", total > "/dev/stderr";
                  }
                }
                
                END {
                  printf "- 精确域名过滤: 保留 %d, 过滤 %d\n", kept, filtered > "/dev/stderr";
                }
                ' "$exact_file" 2>&1 > "$final_exact"
                
                # 合并结果
                cat "$final_wildcards" "$final_exact" | sort -u > "$final_file"
                
                # 清理临时文件
                rm -f "$deduped_wildcards" "$final_wildcards" "$wildcards_processed" "$final_exact"
              else
                # 如果没有泛域名或没有精确域名，直接去重
                echo "没有发现泛域名关系，直接去重..."
                sort -u "$domains_file" > "$final_file"
              fi
              
              # 统计结果
              local final_count=$(wc -l < "$final_file")
              local removed_count=$((cleaned_count - final_count))
              echo "去重后的规则数量: $final_count (减少了 $removed_count 个重复项)"
              
              # 清理临时文件
              rm -f "$domains_file" "$wildcard_file" "$exact_file" "$deduped_file"
              
              # 4. 添加元数据并保存
              echo "正在生成最终规则文件..."
              
              # 创建带有元数据的版本
              local meta_file=$(mktemp)
              
              # 添加元数据
              {
                echo "# 规则来源:"
                for url in $urls; do
                  # 转换为GitHub仓库URL显示
                  repo_url=$(echo "$url" | sed -E 's|raw.githubusercontent.com/([^/]+/[^/]+).*|github.com/\1|')
                  echo "# - https://$repo_url"
                done
                echo ""
                # 添加规则内容
                cat "$final_file"
              } > "$meta_file"
              
              # 5. 检查是否有更改
              local changed=0
              if [ -f "$output_path" ]; then
                # 比较旧文件和新文件(忽略更新时间)
                local old_file=$(mktemp)
                grep -v "^# Update time:" "$output_path" > "$old_file"
                
                if ! cmp -s "$old_file" "$meta_file"; then
                  changed=1  # 文件有变化
                  # 计算增加和删除的规则数量
                  local old_lines=$(grep -v "^#" "$old_file" | wc -l)
                  local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                  local added=$((new_lines > old_lines ? new_lines - old_lines : 0))
                  local removed=$((old_lines > new_lines ? old_lines - new_lines : 0))
                  
                  echo "规则变化统计:"
                  echo "- 原有规则数量: $old_lines"
                  echo "- 新规则数量: $new_lines"
                  echo "- 变化: 新增 $added 条, 移除 $removed 条"
                fi
                rm -f "$old_file"
              else
                changed=1  # 文件不存在,需要创建
                local new_lines=$(grep -v "^#" "$meta_file" | wc -l)
                echo "新建规则文件，包含 $new_lines 条规则"
              fi
              
              # 如果有更改，写入新文件
              if [ $changed -eq 1 ]; then
                {
                  echo "# Update time: $(date '+%Y-%m-%d %H:%M:%S')"
                  cat "$meta_file"
                } > "$output_path"
                echo "规则已更新"
              else
                echo "规则无变化，无需更新"
              fi
              
              # 清理临时文件
              rm -f "$final_file" "$meta_file"
            else
              echo "警告: 没有找到有效内容，跳过处理"
            fi
            
            # 删除所有临时文件
            rm -f "$merged_file" "$cleaned_file"
            rm -rf "$tmp_dir"
            
            # 显示处理用时
            local duration=$((SECONDS - start_time))
            echo "处理用时: $duration 秒"
          }

          # 主程序 - 控制整个处理流程
          main() {
            echo "===== 网络规则更新程序 ====="
            echo "开始时间: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "============================"
            
            # 初始化数据结构
            declare -A rule_configs  # 存储规则配置
            declare -a rule_files    # 存储规则文件路径
            
            # 获取所有规则配置
            get_rules_config rule_configs rule_files
            
            # 如果没有找到规则，就退出
            if [ ${#rule_configs[@]} -eq 0 ]; then
              echo "没有找到规则配置，程序结束"
              return
            fi
            
            echo "============================"
            echo "开始更新规则"
            echo "============================"
            
            # 记录开始时间
            local start_time=$SECONDS
            
            # 处理每个规则
            for rule_name in "${!rule_configs[@]}"; do
              # 分离路径和URL
              IFS=';' read -r output_path urls <<< "${rule_configs[$rule_name]}"
              
              echo "============================"
              # 处理规则
              process_rule "$rule_name" "$output_path" "$urls"
              echo "============================"
            done
            
            # 计算总耗时
            local duration=$((SECONDS - start_time))
            echo "所有规则处理完成"
            echo "总用时: $((duration / 60))分$((duration % 60))秒"
            
            # 检测是否有变化
            local has_changes=false
            local change_summary=""
            local total_added=0
            local total_removed=0
            
            # 添加文件到Git以检测变化
            git add "${rule_files[@]}" 2>/dev/null || true
            
            # 检查每个文件是否有实质变化
            for file in "${rule_files[@]}"; do
              if [ -f "$file" ] && git diff --cached --no-color "$file" | 
                 grep -v '^[+-]# Update time:' | grep -q '^[+-]'; then
                has_changes=true
                local basename=$(basename "$file")
                
                # 计算具体变化的行数
                local added_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^+')
                local removed_lines=$(git diff --cached --no-color "$file" | grep -v '^[+-]# ' | grep -c '^-')
                
                echo "文件有变化: $file"
                echo "- 详细统计: 新增 $added_lines 条规则, 移除 $removed_lines 条规则"
                
                # 添加到变化摘要
                change_summary="${change_summary}[$basename] +$added_lines/-$removed_lines "
                
                # 累计总变化量
                total_added=$((total_added + added_lines))
                total_removed=$((total_removed + removed_lines))
              fi
            done
            
            # 添加总计到变化摘要
            if [ "$has_changes" = true ]; then
              change_summary="${change_summary}[总计] +$total_added/-$total_removed"
              echo "总变化: 新增 $total_added 条规则, 移除 $total_removed 条规则"
            fi
            
            # 设置输出变量
            if [ "$has_changes" = true ]; then
              echo "has_changes=true" >> "$GITHUB_OUTPUT"
              echo "change_summary=$change_summary" >> "$GITHUB_OUTPUT"
              echo "规则已更新，即将提交更改"
            else
              echo "has_changes=false" >> "$GITHUB_OUTPUT"
              echo "规则无变化，无需提交"
              # 还原已暂存的文件
              git restore --staged "${rule_files[@]}" 2>/dev/null || true
            fi
            
            echo "============================"
            echo "程序结束"
            echo "============================"
          }

          # 运行主程序
          main
EOF
          
          chmod +x process_rules.sh
          echo "规则处理脚本已生成，权限设置为可执行"
          echo "脚本生成完成，将执行规则更新..."

      - name: Update RuleSets
        id: check_changes
        run: |
          echo "开始执行规则更新流程..."
          ./process_rules.sh

      - name: Commit and Push Changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "检测到规则变更，准备提交..."
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # 使用变化摘要作为提交消息
          git commit -m "Auto update rules: ${{ steps.check_changes.outputs.change_summary }}"
          
          # 显示本次提交的统计信息
          echo "提交变更统计:"
          git show --stat --oneline HEAD
          
          echo "推送变更到仓库..."
          git push
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Delete Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 2  # 保留最近2次运行记录，便于查看历史
